{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-27T07:22:32.076213Z",
     "start_time": "2025-02-27T07:22:32.066844Z"
    }
   },
   "source": [
    "from lxml import etree\n",
    "test_file = 'data3/0a961053-DESKTOP-svg-clean.svg'\n",
    "# Open file\n",
    "with open(test_file, 'r') as file:\n",
    "    data = file.read()"
   ],
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T07:22:33.218284Z",
     "start_time": "2025-02-27T07:22:33.211262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "def data_to_pattern(data):\n",
    "    # Repeat the pattern \"A \" as many times as it's needed to match the data length.\n",
    "    pattern = \"A \" * (len(data) // 2)\n",
    "    if len(data) % 2 == 1:\n",
    "        pattern += \"A\"\n",
    "    return pattern\n",
    "\n",
    "class CollectorTarget(object):\n",
    "    def __init__(self):\n",
    "        self.tags = Counter()\n",
    "        self.attributes = Counter()\n",
    "        self.attribute_values = list()\n",
    "        self.data_values = list()\n",
    "    \n",
    "    def start(self, tag, attrib):\n",
    "        if '{' == tag[0]:\n",
    "            tag = tag.split('}')[-1]\n",
    "        self.tags.update([tag])\n",
    "        self.attributes.update(attrib.keys())\n",
    "        \n",
    "        for key, value in attrib.items():\n",
    "            if not self._is_image(key, value):     \n",
    "                self.attribute_values.append(value)\n",
    "    def end(self, tag):\n",
    "        pass\n",
    "    def data(self, data):\n",
    "        pass\n",
    "        # self.data_values.append(data_to_pattern(data))\n",
    "    def comment(self, text):\n",
    "        pass\n",
    "    def close(self):\n",
    "        pass\n",
    "    def _is_image(self, attrib_key, attrib_value):\n",
    "        if  '.' in attrib_value:\n",
    "            extension = attrib_value.split('.')[-1]\n",
    "            if '?' in extension:\n",
    "                extension = extension.split('?')[0]\n",
    "            return extension.lower() in ['jpg', 'jpeg', 'png', 'gif', 'svg', 'webp']\n",
    "\n",
    "parser = etree.XMLParser(target = CollectorTarget())\n",
    "\n",
    "result = etree.XML('<br />', parser)"
   ],
   "id": "a28472e1db281f27",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T07:22:35.752290Z",
     "start_time": "2025-02-27T07:22:35.065941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from lxml.etree import XMLSyntaxError\n",
    "import glob\n",
    "\n",
    "files = glob.glob('data3/*')\n",
    "all_files = [file for file in files if file.endswith('svg-clean.svg')]\n",
    "\n",
    "files = all_files[:100]\n",
    "target = CollectorTarget()\n",
    "\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        data = f.read()\n",
    "        try:\n",
    "            parser = etree.XMLParser(target = target)\n",
    "            result = etree.XML(data, parser)\n",
    "        except XMLSyntaxError as e:\n",
    "            print(file)\n",
    "            print(e)\n",
    "        \n",
    "# print(target.tags.most_common())\n",
    "# print(target.attributes.most_common())\n",
    "# print(target.attribute_values[:100])"
   ],
   "id": "ab309e4ce2e84b2a",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T07:22:38.557976Z",
     "start_time": "2025-02-27T07:22:37.047474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "\n",
    "text_tokenizer = ByteLevelBPETokenizer()\n",
    "text_tokenizer.train_from_iterator([target.attribute_values + target.data_values], vocab_size=5000, min_frequency=2)"
   ],
   "id": "942ec7e555e14b1c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T07:22:41.647424Z",
     "start_time": "2025-02-27T07:22:41.634352Z"
    }
   },
   "cell_type": "code",
   "source": "text_tokenizer.encode(\"childStackingContextsWithStackLevelZeroAndPositionedDescendantsWithStackLevelZero\")",
   "id": "d3eb2ff893a28759",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=1, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T07:26:54.575230Z",
     "start_time": "2025-02-27T07:26:54.556262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TokenizerTarget(object):\n",
    "    def __init__(self, common_tags, common_attributes, text_tokenizer):\n",
    "        # Todo: use faster data types here.\n",
    "        self.common_tags = common_tags\n",
    "        self.common_attributes = common_attributes\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        self.tokenized_image_urls = list()\n",
    "        self.current_tokens = list()\n",
    "        self.last_tokens = list()\n",
    "        self.special_tokens = ['close tag', 'data starts']\n",
    "        self.data_tokens = [pow(2, i) for i in range(30, -1, -1)]\n",
    "        \n",
    "        self.offsets = {}\n",
    "        self.offsets['special_tokens'] = 0\n",
    "        self.offsets['data_tokens'] = self.offsets['special_tokens'] + len(self.special_tokens)\n",
    "        self.offsets['common_tags'] = self.offsets['special_tokens'] + len(self.data_tokens)\n",
    "        self.offsets['common_attributes'] = self.offsets['common_tags'] + len(self.common_tags)\n",
    "        self.offsets['text_tokens'] = self.offsets['common_attributes'] + len(self.common_attributes)\n",
    "        self.offsets['image_urls'] = self.offsets['text_tokens'] + text_tokenizer.get_vocab_size()\n",
    "    \n",
    "    def start(self, tag, attrib):\n",
    "        if '{' == tag[0]:\n",
    "            tag = tag.split('}')[-1]\n",
    "        self._tokenize_tag(tag)\n",
    "        \n",
    "        for key, value in attrib.items():\n",
    "            if self._is_image(key, value):\n",
    "                self._tokenize_image_attribute(key, value)\n",
    "            else:\n",
    "                self._tokenize_attribute(key, value)\n",
    "    def end(self, tag):\n",
    "        self.current_tokens.append(self.offsets['special_tokens'] + self.special_tokens.index('close tag'))\n",
    "        pass\n",
    "    def data(self, data):\n",
    "        self.current_tokens.append(self.offsets['special_tokens'] + self.special_tokens.index('data starts'))\n",
    "        self._tokenize_text(data)\n",
    "        # self._tokenize_data(data)\n",
    "        pass\n",
    "    def comment(self, text):\n",
    "        pass\n",
    "    def close(self):\n",
    "        self.last_tokens = self.current_tokens\n",
    "        self.current_tokens = list()\n",
    "\n",
    "    def _is_image(self, attrib_key, attrib_value):\n",
    "        if  '.' in attrib_value:\n",
    "            extension = attrib_value.split('.')[-1]\n",
    "            if '?' in extension:\n",
    "                extension = extension.split('?')[0]\n",
    "            return extension.lower() in ['jpg', 'jpeg', 'png', 'gif', 'svg', 'webp']\n",
    "\n",
    "    def _tokenize_tag(self, tag):\n",
    "        if tag not in self.common_tags:\n",
    "            raise Exception('Tag not in common tags: ' + tag)\n",
    "        self.current_tokens.append(self.offsets['common_tags'] + self.common_tags.index(tag))\n",
    "\n",
    "    def _tokenize_text(self, text):\n",
    "        encoded_text = self.text_tokenizer.encode(text).ids\n",
    "        self.current_tokens.extend([self.offsets['text_tokens'] + token for token in encoded_text])\n",
    "\n",
    "    def _tokenize_attribute(self, key, value):\n",
    "        if key not in self.common_attributes:\n",
    "            raise Exception('Attribute not in common attributes: ' + key)\n",
    "        self.current_tokens.append(self.offsets['common_attributes'] + self.common_attributes.index(key))\n",
    "        self._tokenize_text(value)\n",
    "\n",
    "    def _tokenize_image_attribute(self, key, value):\n",
    "        if key not in self.common_attributes:\n",
    "            raise Exception('Image attribute not in common attributes: ' + key)\n",
    "\n",
    "        self.current_tokens.append(self.offsets['common_attributes'] + self.common_attributes.index(key))\n",
    "        \n",
    "        if value not in self.tokenized_image_urls:\n",
    "            self.tokenized_image_urls.append(value)\n",
    "        self.current_tokens.append(self.offsets['image_urls'] + self.tokenized_image_urls.index(value))\n",
    "\n",
    "    def _tokenize_data(self, data):\n",
    "        data_length = len(data)\n",
    "        for token, token_length in enumerate(self.data_tokens):\n",
    "            if data_length <= token_length:\n",
    "                self.current_tokens.append(token)\n",
    "                data_length -= token_length\n",
    "                if data_length == 0:\n",
    "                    break\n",
    "                    \n",
    "                    \n",
    "    # ----------------\n",
    "    \n",
    "    def decode(self, tokens):\n",
    "        decoded = []\n",
    "        open_tags = list()\n",
    "\n",
    "        class ParsingMode:\n",
    "            IN_TAG = 0\n",
    "            ATTRIBUTE_STARTED = 1\n",
    "            IN_ATTRIBUTE = 2\n",
    "            DATA = 3\n",
    "            \n",
    "        current_parsing_mode = ParsingMode.DATA\n",
    "        \n",
    "        for token in tokens:\n",
    "            if token >= self.offsets['image_urls']:\n",
    "                decoded.append(self._decode_image_url_token(token))\n",
    "            elif token >= self.offsets['text_tokens']:\n",
    "                if current_parsing_mode == ParsingMode.ATTRIBUTE_STARTED:\n",
    "                    decoded.append('=\"')\n",
    "                    current_parsing_mode = ParsingMode.IN_ATTRIBUTE\n",
    "                decoded.append(self._decode_text_token(token))\n",
    "            elif token >= self.offsets['common_attributes']:\n",
    "                if current_parsing_mode == ParsingMode.ATTRIBUTE_STARTED:\n",
    "                    decoded.append('=\"\" ')\n",
    "                if current_parsing_mode == ParsingMode.IN_ATTRIBUTE:\n",
    "                    decoded.append('\" ')\n",
    "                if current_parsing_mode == ParsingMode.IN_TAG:\n",
    "                    decoded.append(' ')\n",
    "                current_parsing_mode = ParsingMode.ATTRIBUTE_STARTED\n",
    "                decoded.append(self._decode_attribute_token(token))\n",
    "            elif token >= self.offsets['common_tags']:\n",
    "                if current_parsing_mode == ParsingMode.ATTRIBUTE_STARTED:\n",
    "                    decoded.append('=\"\">')\n",
    "                if current_parsing_mode == ParsingMode.IN_ATTRIBUTE:\n",
    "                    decoded.append('\">')\n",
    "                if current_parsing_mode == ParsingMode.IN_TAG:\n",
    "                    decoded.append('>')\n",
    "                current_tag = self._decode_tag_token(token)\n",
    "                open_tags.append(current_tag)\n",
    "                decoded.append('<' + current_tag)\n",
    "                current_parsing_mode = ParsingMode.IN_TAG\n",
    "            elif token >= self.offsets['data_tokens']:\n",
    "                decoded.append(self._decode_data_token(token))\n",
    "            else:\n",
    "                special_token = self._decode_special_token(token)\n",
    "                if special_token == 'close tag':\n",
    "                    if current_parsing_mode != ParsingMode.DATA:\n",
    "                        if current_parsing_mode == ParsingMode.ATTRIBUTE_STARTED:\n",
    "                            decoded.append('=\"\"')\n",
    "                        if current_parsing_mode == ParsingMode.IN_ATTRIBUTE:\n",
    "                            decoded.append('\"')\n",
    "                        decoded.append('>')\n",
    "                    current_tag = open_tags.pop()\n",
    "                    decoded.append('</' + current_tag + '>')\n",
    "                    current_parsing_mode = ParsingMode.DATA\n",
    "                if special_token == 'data starts':\n",
    "                    if current_parsing_mode == ParsingMode.ATTRIBUTE_STARTED:\n",
    "                        decoded.append('=\"\"')\n",
    "                    if current_parsing_mode == ParsingMode.IN_ATTRIBUTE:\n",
    "                        decoded.append('\"')\n",
    "                    decoded.append('>')\n",
    "                    current_parsing_mode = ParsingMode.DATA\n",
    "        return \"\".join(decoded)\n",
    "\n",
    "    def _decode_special_token(self, token):\n",
    "        return self.special_tokens[token - self.offsets['special_tokens']]\n",
    "\n",
    "    def _decode_data_token(self, token):\n",
    "        return 'data'\n",
    "\n",
    "    def _decode_tag_token(self, token):\n",
    "        return self.common_tags[token - self.offsets['common_tags']]\n",
    "\n",
    "    def _decode_attribute_token(self, token):\n",
    "        return self.common_attributes[token - self.offsets['common_attributes']]\n",
    "\n",
    "    def _decode_text_token(self, token):\n",
    "        return self.text_tokenizer.decode([token - self.offsets['text_tokens']])\n",
    "\n",
    "    def _decode_image_url_token(self, token):\n",
    "        return self.tokenized_image_urls[token - self.offsets['image_urls']]\n",
    "\n"
   ],
   "id": "8cfe16a2fc6ca73f",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T07:26:55.366843Z",
     "start_time": "2025-02-27T07:26:55.347713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_file = 'data3/0a961053-DESKTOP-svg-clean.svg'\n",
    "\n",
    "tokenizer_target = TokenizerTarget(\n",
    "    [item for item, count in target.tags.most_common(500)], \n",
    "    [item for item, count in target.attributes.most_common(5000)],\n",
    "    text_tokenizer\n",
    ")\n",
    "\n",
    "# Open file\n",
    "with open(test_file, 'r') as file:\n",
    "    data = file.read()\n",
    "\n",
    "    try:\n",
    "        parser = etree.XMLParser(target = tokenizer_target)\n",
    "        result = etree.XML(data, parser)\n",
    "        \n",
    "        print(len(tokenizer_target.last_tokens))\n",
    "        print(len(data))\n",
    "    except XMLSyntaxError as e:\n",
    "        print(file)\n",
    "        print(e)"
   ],
   "id": "7dd6b59324167d8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5926\n",
      "33023\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T07:27:20.243183Z",
     "start_time": "2025-02-27T07:27:20.217861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "parser = etree.XMLParser(target = tokenizer_target)\n",
    "etree.XML('<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"1440\" height=\"1148\" viewBox=\"0 0 1440 1148\"><text color=\"rgb(255, 255, 255)\" dominant-baseline=\"text-after-edge\" font-family=\"Icons\" font-size=\"14.4px\" font-stretch=\"100%\" font-style=\"normal\" font-variant=\"normal\" font-weight=\"400\" direction=\"ltr\" letter-spacing=\"normal\" text-decoration=\"none solid rgb(255, 255, 255)\" text-anchor=\"start\" text-rendering=\"auto\" unicode-bidi=\"normal\" word-spacing=\"0px\" writing-mode=\"horizontal-tb\" user-select=\"none\" fill=\"rgb(255, 255, 255)\">Hello</text><text></text><tspan>AAA</tspan><tspan/></svg>', parser)\n",
    "\n",
    "tokens = tokenizer_target.last_tokens\n",
    "print(tokens)"
   ],
   "id": "5519f39b0ae2587b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43, 65, 1764, 66, 478, 671, 109, 162, 435, 4791, 462, 1956, 34, 67, 424, 154, 460, 158, 509, 158, 509, 155, 68, 441, 159, 544, 159, 532, 69, 3720, 70, 518, 160, 166, 452, 71, 521, 151, 72, 431, 73, 431, 74, 633, 75, 549, 76, 431, 77, 517, 552, 551, 154, 460, 158, 509, 158, 509, 155, 78, 542, 79, 411, 80, 431, 81, 162, 452, 82, 547, 159, 533, 83, 517, 60, 424, 154, 460, 158, 509, 158, 509, 155, 1, 186, 413, 466, 0, 34, 0, 32, 1, 4487, 0, 32, 0, 0]\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T07:27:21.425220Z",
     "start_time": "2025-02-27T07:27:21.422562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "decoded = tokenizer_target.decode(tokens)\n",
    "print(decoded)"
   ],
   "id": "894c8748ae81c5f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<svg width=\"1440\" height=\"1148\" viewBox=\"0 0 1440 1148\"><text color=\"rgb(255, 255, 255)\" dominant-baseline=\"text-after-edge\" font-family=\"Icons\" font-size=\"14.4px\" font-stretch=\"100%\" font-style=\"normal\" font-variant=\"normal\" font-weight=\"400\" direction=\"ltr\" letter-spacing=\"normal\" text-decoration=\"none solid rgb(255, 255, 255)\" text-anchor=\"start\" text-rendering=\"auto\" unicode-bidi=\"normal\" word-spacing=\"0px\" writing-mode=\"horizontal-tb\" user-select=\"none\" fill=\"rgb(255, 255, 255)\">Hello</text><text></text><tspan>AAA</tspan><tspan></tspan></svg>\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T07:38:45.818011Z",
     "start_time": "2025-02-27T07:38:45.810996Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cssutils\n",
    "\n",
    "style = cssutils.parseString('a { color: rgb(255, 255, 255); }')\n",
    "print(style.cssRules[0].selectorText)\n",
    "print(style.cssRules[0].selectorText)"
   ],
   "id": "4172055946007895",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c1afb648214eaa58"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
