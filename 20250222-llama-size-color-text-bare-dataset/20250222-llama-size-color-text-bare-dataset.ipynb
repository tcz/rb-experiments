{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5467970cf69d792",
   "metadata": {},
   "source": [
    "# Llama 3.2 fine tuning with size-color-text \"bare\" dataset\n",
    "\n",
    "2025-02-22 10:10\n",
    "\n",
    "After the failures to fine-tune with the size-color-text, I created a new dataset that removed all the repeating, matching parts of the SVG and markup and reduced the differences to a set of parameters organized in a JSON strong. Did 3 days of fine-funing but unfortunately I only got repeating garbage. It might have to do with the learning rate although not sure. I'm running another batch with a different learning rate and batch size. The batch size here was kept low because I reused the notebook from the size-color-text full dataset and its longer sequences but it could have been increased. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c8567da-339d-42d8-a672-fd9474f75a04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  binutils binutils-common binutils-x86-64-linux-gnu bzip2 cpp cpp-11 dirmngr\n",
      "  dpkg-dev fakeroot g++ g++-11 gcc gcc-11 gcc-11-base gnupg gnupg-l10n\n",
      "  gnupg-utils gpg-agent gpg-wks-client gpg-wks-server gpgsm\n",
      "  libalgorithm-diff-perl libalgorithm-diff-xs-perl libalgorithm-merge-perl\n",
      "  libasan6 libatomic1 libbinutils libcc1-0 libctf-nobfd0 libctf0 libdpkg-perl\n",
      "  libfakeroot libfile-fcntllock-perl libgcc-11-dev libgomp1 libisl23 libitm1\n",
      "  libksba8 liblocale-gettext-perl liblsan0 libmpc3 libmpfr6 libnpth0\n",
      "  libquadmath0 libstdc++-11-dev libtsan0 libubsan1 lto-disabled-list make\n",
      "  patch pinentry-curses xz-utils\n",
      "Suggested packages:\n",
      "  binutils-doc bzip2-doc cpp-doc gcc-11-locales dbus-user-session\n",
      "  pinentry-gnome3 tor debian-keyring g++-multilib g++-11-multilib gcc-11-doc\n",
      "  gcc-multilib manpages-dev autoconf automake libtool flex bison gdb gcc-doc\n",
      "  gcc-11-multilib parcimonie xloadimage scdaemon bzr libstdc++-11-doc make-doc\n",
      "  ed diffutils-doc pinentry-doc\n",
      "The following NEW packages will be installed:\n",
      "  binutils binutils-common binutils-x86-64-linux-gnu build-essential bzip2 cpp\n",
      "  cpp-11 dirmngr dpkg-dev fakeroot g++ g++-11 gcc gcc-11 gcc-11-base gnupg\n",
      "  gnupg-l10n gnupg-utils gpg-agent gpg-wks-client gpg-wks-server gpgsm\n",
      "  libalgorithm-diff-perl libalgorithm-diff-xs-perl libalgorithm-merge-perl\n",
      "  libasan6 libatomic1 libbinutils libcc1-0 libctf-nobfd0 libctf0 libdpkg-perl\n",
      "  libfakeroot libfile-fcntllock-perl libgcc-11-dev libgomp1 libisl23 libitm1\n",
      "  libksba8 liblocale-gettext-perl liblsan0 libmpc3 libmpfr6 libnpth0\n",
      "  libquadmath0 libstdc++-11-dev libtsan0 libubsan1 lto-disabled-list make\n",
      "  patch pinentry-curses xz-utils\n",
      "0 upgraded, 53 newly installed, 0 to remove and 42 not upgraded.\n",
      "Need to get 62.3 MB of archives.\n",
      "After this operation, 198 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblocale-gettext-perl amd64 1.07-4build3 [17.1 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 xz-utils amd64 5.2.5-2ubuntu1 [84.8 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 binutils-common amd64 2.38-4ubuntu2.6 [222 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libbinutils amd64 2.38-4ubuntu2.6 [662 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libctf-nobfd0 amd64 2.38-4ubuntu2.6 [108 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libctf0 amd64 2.38-4ubuntu2.6 [103 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 binutils-x86-64-linux-gnu amd64 2.38-4ubuntu2.6 [2326 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 binutils amd64 2.38-4ubuntu2.6 [3200 B]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gcc-11-base amd64 11.4.0-1ubuntu1~22.04 [20.2 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 libisl23 amd64 0.24-2build1 [727 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libmpfr6 amd64 4.1.0-3build3 [1425 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libmpc3 amd64 1.2.1-2build1 [46.9 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 cpp-11 amd64 11.4.0-1ubuntu1~22.04 [10.0 MB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 cpp amd64 4:11.2.0-1ubuntu1 [27.7 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libcc1-0 amd64 12.3.0-1ubuntu1~22.04 [48.3 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgomp1 amd64 12.3.0-1ubuntu1~22.04 [126 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libitm1 amd64 12.3.0-1ubuntu1~22.04 [30.2 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libatomic1 amd64 12.3.0-1ubuntu1~22.04 [10.4 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libasan6 amd64 11.4.0-1ubuntu1~22.04 [2282 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 liblsan0 amd64 12.3.0-1ubuntu1~22.04 [1069 kB]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libtsan0 amd64 11.4.0-1ubuntu1~22.04 [2260 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libubsan1 amd64 12.3.0-1ubuntu1~22.04 [976 kB]\n",
      "Get:23 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libquadmath0 amd64 12.3.0-1ubuntu1~22.04 [154 kB]\n",
      "Get:24 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgcc-11-dev amd64 11.4.0-1ubuntu1~22.04 [2517 kB]\n",
      "Get:25 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gcc-11 amd64 11.4.0-1ubuntu1~22.04 [20.1 MB]\n",
      "Get:26 http://archive.ubuntu.com/ubuntu jammy/main amd64 gcc amd64 4:11.2.0-1ubuntu1 [5112 B]\n",
      "Get:27 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libstdc++-11-dev amd64 11.4.0-1ubuntu1~22.04 [2101 kB]\n",
      "Get:28 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 g++-11 amd64 11.4.0-1ubuntu1~22.04 [11.4 MB]\n",
      "Get:29 http://archive.ubuntu.com/ubuntu jammy/main amd64 g++ amd64 4:11.2.0-1ubuntu1 [1412 B]\n",
      "Get:30 http://archive.ubuntu.com/ubuntu jammy/main amd64 make amd64 4.3-4.1build1 [180 kB]\n",
      "Get:31 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libdpkg-perl all 1.21.1ubuntu2.3 [237 kB]\n",
      "Get:32 http://archive.ubuntu.com/ubuntu jammy/main amd64 bzip2 amd64 1.0.8-5build1 [34.8 kB]\n",
      "Get:33 http://archive.ubuntu.com/ubuntu jammy/main amd64 patch amd64 2.7.6-7build2 [109 kB]\n",
      "Get:34 http://archive.ubuntu.com/ubuntu jammy/main amd64 lto-disabled-list all 24 [12.5 kB]\n",
      "Get:35 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 dpkg-dev all 1.21.1ubuntu2.3 [922 kB]\n",
      "Get:36 http://archive.ubuntu.com/ubuntu jammy/main amd64 build-essential amd64 12.9ubuntu3 [4744 B]\n",
      "Get:37 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libksba8 amd64 1.6.0-2ubuntu0.2 [119 kB]\n",
      "Get:38 http://archive.ubuntu.com/ubuntu jammy/main amd64 libnpth0 amd64 1.6-3build2 [8664 B]\n",
      "Get:39 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 dirmngr amd64 2.2.27-3ubuntu2.1 [293 kB]\n",
      "Get:40 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfakeroot amd64 1.28-1ubuntu1 [31.5 kB]\n",
      "Get:41 http://archive.ubuntu.com/ubuntu jammy/main amd64 fakeroot amd64 1.28-1ubuntu1 [60.4 kB]\n",
      "Get:42 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gnupg-l10n all 2.2.27-3ubuntu2.1 [54.4 kB]\n",
      "Get:43 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gnupg-utils amd64 2.2.27-3ubuntu2.1 [308 kB]\n",
      "Get:44 http://archive.ubuntu.com/ubuntu jammy/main amd64 pinentry-curses amd64 1.1.1-1build2 [34.4 kB]\n",
      "Get:45 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gpg-agent amd64 2.2.27-3ubuntu2.1 [209 kB]\n",
      "Get:46 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gpg-wks-client amd64 2.2.27-3ubuntu2.1 [62.7 kB]\n",
      "Get:47 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gpg-wks-server amd64 2.2.27-3ubuntu2.1 [57.5 kB]\n",
      "Get:48 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gpgsm amd64 2.2.27-3ubuntu2.1 [197 kB]\n",
      "Get:49 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gnupg all 2.2.27-3ubuntu2.1 [315 kB]\n",
      "Get:50 http://archive.ubuntu.com/ubuntu jammy/main amd64 libalgorithm-diff-perl all 1.201-1 [41.8 kB]\n",
      "Get:51 http://archive.ubuntu.com/ubuntu jammy/main amd64 libalgorithm-diff-xs-perl amd64 0.04-6build3 [11.9 kB]\n",
      "Get:52 http://archive.ubuntu.com/ubuntu jammy/main amd64 libalgorithm-merge-perl all 0.08-3 [12.0 kB]\n",
      "Get:53 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfile-fcntllock-perl amd64 0.22-3build7 [33.9 kB]\n",
      "Fetched 62.3 MB in 4s (13.9 MB/s)                   \n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package liblocale-gettext-perl.\n",
      "(Reading database ... 16754 files and directories currently installed.)\n",
      "Preparing to unpack .../00-liblocale-gettext-perl_1.07-4build3_amd64.deb ...\n",
      "Unpacking liblocale-gettext-perl (1.07-4build3) ...\n",
      "Selecting previously unselected package xz-utils.\n",
      "Preparing to unpack .../01-xz-utils_5.2.5-2ubuntu1_amd64.deb ...\n",
      "Unpacking xz-utils (5.2.5-2ubuntu1) ...\n",
      "Selecting previously unselected package binutils-common:amd64.\n",
      "Preparing to unpack .../02-binutils-common_2.38-4ubuntu2.6_amd64.deb ...\n",
      "Unpacking binutils-common:amd64 (2.38-4ubuntu2.6) ...\n",
      "Selecting previously unselected package libbinutils:amd64.\n",
      "Preparing to unpack .../03-libbinutils_2.38-4ubuntu2.6_amd64.deb ...\n",
      "Unpacking libbinutils:amd64 (2.38-4ubuntu2.6) ...\n",
      "Selecting previously unselected package libctf-nobfd0:amd64.\n",
      "Preparing to unpack .../04-libctf-nobfd0_2.38-4ubuntu2.6_amd64.deb ...\n",
      "Unpacking libctf-nobfd0:amd64 (2.38-4ubuntu2.6) ...\n",
      "Selecting previously unselected package libctf0:amd64.\n",
      "Preparing to unpack .../05-libctf0_2.38-4ubuntu2.6_amd64.deb ...\n",
      "Unpacking libctf0:amd64 (2.38-4ubuntu2.6) ...\n",
      "Selecting previously unselected package binutils-x86-64-linux-gnu.\n",
      "Preparing to unpack .../06-binutils-x86-64-linux-gnu_2.38-4ubuntu2.6_amd64.deb ...\n",
      "Unpacking binutils-x86-64-linux-gnu (2.38-4ubuntu2.6) ...\n",
      "Selecting previously unselected package binutils.\n",
      "Preparing to unpack .../07-binutils_2.38-4ubuntu2.6_amd64.deb ...\n",
      "Unpacking binutils (2.38-4ubuntu2.6) ...\n",
      "Selecting previously unselected package gcc-11-base:amd64.\n",
      "Preparing to unpack .../08-gcc-11-base_11.4.0-1ubuntu1~22.04_amd64.deb ...\n",
      "Unpacking gcc-11-base:amd64 (11.4.0-1ubuntu1~22.04) ...\n",
      "Selecting previously unselected package libisl23:amd64.\n",
      "Preparing to unpack .../09-libisl23_0.24-2build1_amd64.deb ...\n",
      "Unpacking libisl23:amd64 (0.24-2build1) ...\n",
      "Selecting previously unselected package libmpfr6:amd64.\n",
      "Preparing to unpack .../10-libmpfr6_4.1.0-3build3_amd64.deb ...\n",
      "Unpacking libmpfr6:amd64 (4.1.0-3build3) ...\n",
      "Selecting previously unselected package libmpc3:amd64.\n",
      "Preparing to unpack .../11-libmpc3_1.2.1-2build1_amd64.deb ...\n",
      "Unpacking libmpc3:amd64 (1.2.1-2build1) ...\n",
      "Selecting previously unselected package cpp-11.\n",
      "Preparing to unpack .../12-cpp-11_11.4.0-1ubuntu1~22.04_amd64.deb ...\n",
      "Unpacking cpp-11 (11.4.0-1ubuntu1~22.04) ...\n",
      "Selecting previously unselected package cpp.\n",
      "Preparing to unpack .../13-cpp_4%3a11.2.0-1ubuntu1_amd64.deb ...\n",
      "Unpacking cpp (4:11.2.0-1ubuntu1) ...\n",
      "Selecting previously unselected package libcc1-0:amd64.\n",
      "Preparing to unpack .../14-libcc1-0_12.3.0-1ubuntu1~22.04_amd64.deb ...\n",
      "Unpacking libcc1-0:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
      "Selecting previously unselected package libgomp1:amd64.\n",
      "Preparing to unpack .../15-libgomp1_12.3.0-1ubuntu1~22.04_amd64.deb ...\n",
      "Unpacking libgomp1:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
      "Selecting previously unselected package libitm1:amd64.\n",
      "Preparing to unpack .../16-libitm1_12.3.0-1ubuntu1~22.04_amd64.deb ...\n",
      "Unpacking libitm1:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
      "Selecting previously unselected package libatomic1:amd64.\n",
      "Preparing to unpack .../17-libatomic1_12.3.0-1ubuntu1~22.04_amd64.deb ...\n",
      "Unpacking libatomic1:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
      "Selecting previously unselected package libasan6:amd64.\n",
      "Preparing to unpack .../18-libasan6_11.4.0-1ubuntu1~22.04_amd64.deb ...\n",
      "Unpacking libasan6:amd64 (11.4.0-1ubuntu1~22.04) ...\n",
      "Selecting previously unselected package liblsan0:amd64.\n",
      "Preparing to unpack .../19-liblsan0_12.3.0-1ubuntu1~22.04_amd64.deb ...\n",
      "Unpacking liblsan0:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
      "Selecting previously unselected package libtsan0:amd64.\n",
      "Preparing to unpack .../20-libtsan0_11.4.0-1ubuntu1~22.04_amd64.deb ...\n",
      "Unpacking libtsan0:amd64 (11.4.0-1ubuntu1~22.04) ...\n",
      "Selecting previously unselected package libubsan1:amd64.\n",
      "Preparing to unpack .../21-libubsan1_12.3.0-1ubuntu1~22.04_amd64.deb ...\n",
      "Unpacking libubsan1:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
      "Selecting previously unselected package libquadmath0:amd64.\n",
      "Preparing to unpack .../22-libquadmath0_12.3.0-1ubuntu1~22.04_amd64.deb ...\n",
      "Unpacking libquadmath0:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
      "Selecting previously unselected package libgcc-11-dev:amd64.\n",
      "Preparing to unpack .../23-libgcc-11-dev_11.4.0-1ubuntu1~22.04_amd64.deb ...\n",
      "Unpacking libgcc-11-dev:amd64 (11.4.0-1ubuntu1~22.04) ...\n",
      "Selecting previously unselected package gcc-11.\n",
      "Preparing to unpack .../24-gcc-11_11.4.0-1ubuntu1~22.04_amd64.deb ...\n",
      "Unpacking gcc-11 (11.4.0-1ubuntu1~22.04) ...\n",
      "Selecting previously unselected package gcc.\n",
      "Preparing to unpack .../25-gcc_4%3a11.2.0-1ubuntu1_amd64.deb ...\n",
      "Unpacking gcc (4:11.2.0-1ubuntu1) ...\n",
      "Selecting previously unselected package libstdc++-11-dev:amd64.\n",
      "Preparing to unpack .../26-libstdc++-11-dev_11.4.0-1ubuntu1~22.04_amd64.deb ...\n",
      "Unpacking libstdc++-11-dev:amd64 (11.4.0-1ubuntu1~22.04) ...\n",
      "Selecting previously unselected package g++-11.\n",
      "Preparing to unpack .../27-g++-11_11.4.0-1ubuntu1~22.04_amd64.deb ...\n",
      "Unpacking g++-11 (11.4.0-1ubuntu1~22.04) ...\n",
      "Selecting previously unselected package g++.\n",
      "Preparing to unpack .../28-g++_4%3a11.2.0-1ubuntu1_amd64.deb ...\n",
      "Unpacking g++ (4:11.2.0-1ubuntu1) ...\n",
      "Selecting previously unselected package make.\n",
      "Preparing to unpack .../29-make_4.3-4.1build1_amd64.deb ...\n",
      "Unpacking make (4.3-4.1build1) ...\n",
      "Selecting previously unselected package libdpkg-perl.\n",
      "Preparing to unpack .../30-libdpkg-perl_1.21.1ubuntu2.3_all.deb ...\n",
      "Unpacking libdpkg-perl (1.21.1ubuntu2.3) ...\n",
      "Selecting previously unselected package bzip2.\n",
      "Preparing to unpack .../31-bzip2_1.0.8-5build1_amd64.deb ...\n",
      "Unpacking bzip2 (1.0.8-5build1) ...\n",
      "Selecting previously unselected package patch.\n",
      "Preparing to unpack .../32-patch_2.7.6-7build2_amd64.deb ...\n",
      "Unpacking patch (2.7.6-7build2) ...\n",
      "Selecting previously unselected package lto-disabled-list.\n",
      "Preparing to unpack .../33-lto-disabled-list_24_all.deb ...\n",
      "Unpacking lto-disabled-list (24) ...\n",
      "Selecting previously unselected package dpkg-dev.\n",
      "Preparing to unpack .../34-dpkg-dev_1.21.1ubuntu2.3_all.deb ...\n",
      "Unpacking dpkg-dev (1.21.1ubuntu2.3) ...\n",
      "Selecting previously unselected package build-essential.\n",
      "Preparing to unpack .../35-build-essential_12.9ubuntu3_amd64.deb ...\n",
      "Unpacking build-essential (12.9ubuntu3) ...\n",
      "Selecting previously unselected package libksba8:amd64.\n",
      "Preparing to unpack .../36-libksba8_1.6.0-2ubuntu0.2_amd64.deb ...\n",
      "Unpacking libksba8:amd64 (1.6.0-2ubuntu0.2) ...\n",
      "Selecting previously unselected package libnpth0:amd64.\n",
      "Preparing to unpack .../37-libnpth0_1.6-3build2_amd64.deb ...\n",
      "Unpacking libnpth0:amd64 (1.6-3build2) ...\n",
      "Selecting previously unselected package dirmngr.\n",
      "Preparing to unpack .../38-dirmngr_2.2.27-3ubuntu2.1_amd64.deb ...\n",
      "Unpacking dirmngr (2.2.27-3ubuntu2.1) ...\n",
      "Selecting previously unselected package libfakeroot:amd64.\n",
      "Preparing to unpack .../39-libfakeroot_1.28-1ubuntu1_amd64.deb ...\n",
      "Unpacking libfakeroot:amd64 (1.28-1ubuntu1) ...\n",
      "Selecting previously unselected package fakeroot.\n",
      "Preparing to unpack .../40-fakeroot_1.28-1ubuntu1_amd64.deb ...\n",
      "Unpacking fakeroot (1.28-1ubuntu1) ...\n",
      "Selecting previously unselected package gnupg-l10n.\n",
      "Preparing to unpack .../41-gnupg-l10n_2.2.27-3ubuntu2.1_all.deb ...\n",
      "Unpacking gnupg-l10n (2.2.27-3ubuntu2.1) ...\n",
      "Selecting previously unselected package gnupg-utils.\n",
      "Preparing to unpack .../42-gnupg-utils_2.2.27-3ubuntu2.1_amd64.deb ...\n",
      "Unpacking gnupg-utils (2.2.27-3ubuntu2.1) ...\n",
      "Selecting previously unselected package pinentry-curses.\n",
      "Preparing to unpack .../43-pinentry-curses_1.1.1-1build2_amd64.deb ...\n",
      "Unpacking pinentry-curses (1.1.1-1build2) ...\n",
      "Selecting previously unselected package gpg-agent.\n",
      "Preparing to unpack .../44-gpg-agent_2.2.27-3ubuntu2.1_amd64.deb ...\n",
      "Unpacking gpg-agent (2.2.27-3ubuntu2.1) ...\n",
      "Selecting previously unselected package gpg-wks-client.\n",
      "Preparing to unpack .../45-gpg-wks-client_2.2.27-3ubuntu2.1_amd64.deb ...\n",
      "Unpacking gpg-wks-client (2.2.27-3ubuntu2.1) ...\n",
      "Selecting previously unselected package gpg-wks-server.\n",
      "Preparing to unpack .../46-gpg-wks-server_2.2.27-3ubuntu2.1_amd64.deb ...\n",
      "Unpacking gpg-wks-server (2.2.27-3ubuntu2.1) ...\n",
      "Selecting previously unselected package gpgsm.\n",
      "Preparing to unpack .../47-gpgsm_2.2.27-3ubuntu2.1_amd64.deb ...\n",
      "Unpacking gpgsm (2.2.27-3ubuntu2.1) ...\n",
      "Selecting previously unselected package gnupg.\n",
      "Preparing to unpack .../48-gnupg_2.2.27-3ubuntu2.1_all.deb ...\n",
      "Unpacking gnupg (2.2.27-3ubuntu2.1) ...\n",
      "Selecting previously unselected package libalgorithm-diff-perl.\n",
      "Preparing to unpack .../49-libalgorithm-diff-perl_1.201-1_all.deb ...\n",
      "Unpacking libalgorithm-diff-perl (1.201-1) ...\n",
      "Selecting previously unselected package libalgorithm-diff-xs-perl.\n",
      "Preparing to unpack .../50-libalgorithm-diff-xs-perl_0.04-6build3_amd64.deb ...\n",
      "Unpacking libalgorithm-diff-xs-perl (0.04-6build3) ...\n",
      "Selecting previously unselected package libalgorithm-merge-perl.\n",
      "Preparing to unpack .../51-libalgorithm-merge-perl_0.08-3_all.deb ...\n",
      "Unpacking libalgorithm-merge-perl (0.08-3) ...\n",
      "Selecting previously unselected package libfile-fcntllock-perl.\n",
      "Preparing to unpack .../52-libfile-fcntllock-perl_0.22-3build7_amd64.deb ...\n",
      "Unpacking libfile-fcntllock-perl (0.22-3build7) ...\n",
      "Setting up libksba8:amd64 (1.6.0-2ubuntu0.2) ...\n",
      "Setting up pinentry-curses (1.1.1-1build2) ...\n",
      "Setting up gcc-11-base:amd64 (11.4.0-1ubuntu1~22.04) ...\n",
      "Setting up lto-disabled-list (24) ...\n",
      "Setting up libfile-fcntllock-perl (0.22-3build7) ...\n",
      "Setting up libalgorithm-diff-perl (1.201-1) ...\n",
      "Setting up gpgsm (2.2.27-3ubuntu2.1) ...\n",
      "Setting up binutils-common:amd64 (2.38-4ubuntu2.6) ...\n",
      "Setting up libctf-nobfd0:amd64 (2.38-4ubuntu2.6) ...\n",
      "Setting up libnpth0:amd64 (1.6-3build2) ...\n",
      "Setting up libgomp1:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
      "Setting up bzip2 (1.0.8-5build1) ...\n",
      "Setting up libfakeroot:amd64 (1.28-1ubuntu1) ...\n",
      "Setting up libasan6:amd64 (11.4.0-1ubuntu1~22.04) ...\n",
      "Setting up fakeroot (1.28-1ubuntu1) ...\n",
      "update-alternatives: using /usr/bin/fakeroot-sysv to provide /usr/bin/fakeroot (fakeroot) in auto mode\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/fakeroot.1.gz because associated file /usr/share/man/man1/fakeroot-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/faked.1.gz because associated file /usr/share/man/man1/faked-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/es/man1/fakeroot.1.gz because associated file /usr/share/man/es/man1/fakeroot-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/es/man1/faked.1.gz because associated file /usr/share/man/es/man1/faked-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/fr/man1/fakeroot.1.gz because associated file /usr/share/man/fr/man1/fakeroot-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/fr/man1/faked.1.gz because associated file /usr/share/man/fr/man1/faked-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/sv/man1/fakeroot.1.gz because associated file /usr/share/man/sv/man1/fakeroot-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/sv/man1/faked.1.gz because associated file /usr/share/man/sv/man1/faked-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "Setting up dirmngr (2.2.27-3ubuntu2.1) ...\n",
      "Created symlink /etc/systemd/user/sockets.target.wants/dirmngr.socket → /usr/lib/systemd/user/dirmngr.socket.\n",
      "Setting up make (4.3-4.1build1) ...\n",
      "Setting up libmpfr6:amd64 (4.1.0-3build3) ...\n",
      "Setting up gnupg-l10n (2.2.27-3ubuntu2.1) ...\n",
      "Setting up xz-utils (5.2.5-2ubuntu1) ...\n",
      "update-alternatives: using /usr/bin/xz to provide /usr/bin/lzma (lzma) in auto mode\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzma.1.gz because associated file /usr/share/man/man1/xz.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/unlzma.1.gz because associated file /usr/share/man/man1/unxz.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzcat.1.gz because associated file /usr/share/man/man1/xzcat.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzmore.1.gz because associated file /usr/share/man/man1/xzmore.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzless.1.gz because associated file /usr/share/man/man1/xzless.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzdiff.1.gz because associated file /usr/share/man/man1/xzdiff.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzcmp.1.gz because associated file /usr/share/man/man1/xzcmp.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzgrep.1.gz because associated file /usr/share/man/man1/xzgrep.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzegrep.1.gz because associated file /usr/share/man/man1/xzegrep.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzfgrep.1.gz because associated file /usr/share/man/man1/xzfgrep.1.gz (of link group lzma) doesn't exist\n",
      "Setting up libquadmath0:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
      "Setting up libmpc3:amd64 (1.2.1-2build1) ...\n",
      "Setting up libatomic1:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
      "Setting up patch (2.7.6-7build2) ...\n",
      "Setting up libdpkg-perl (1.21.1ubuntu2.3) ...\n",
      "Setting up libubsan1:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
      "Setting up libbinutils:amd64 (2.38-4ubuntu2.6) ...\n",
      "Setting up libisl23:amd64 (0.24-2build1) ...\n",
      "Setting up libalgorithm-diff-xs-perl (0.04-6build3) ...\n",
      "Setting up libcc1-0:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
      "Setting up liblocale-gettext-perl (1.07-4build3) ...\n",
      "Setting up liblsan0:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
      "Setting up libitm1:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
      "Setting up libalgorithm-merge-perl (0.08-3) ...\n",
      "Setting up gnupg-utils (2.2.27-3ubuntu2.1) ...\n",
      "Setting up libtsan0:amd64 (11.4.0-1ubuntu1~22.04) ...\n",
      "Setting up libctf0:amd64 (2.38-4ubuntu2.6) ...\n",
      "Setting up cpp-11 (11.4.0-1ubuntu1~22.04) ...\n",
      "Setting up gpg-agent (2.2.27-3ubuntu2.1) ...\n",
      "Created symlink /etc/systemd/user/sockets.target.wants/gpg-agent-browser.socket → /usr/lib/systemd/user/gpg-agent-browser.socket.\n",
      "Created symlink /etc/systemd/user/sockets.target.wants/gpg-agent-extra.socket → /usr/lib/systemd/user/gpg-agent-extra.socket.\n",
      "Created symlink /etc/systemd/user/sockets.target.wants/gpg-agent-ssh.socket → /usr/lib/systemd/user/gpg-agent-ssh.socket.\n",
      "Created symlink /etc/systemd/user/sockets.target.wants/gpg-agent.socket → /usr/lib/systemd/user/gpg-agent.socket.\n",
      "Setting up gpg-wks-client (2.2.27-3ubuntu2.1) ...\n",
      "Setting up gpg-wks-server (2.2.27-3ubuntu2.1) ...\n",
      "Setting up libgcc-11-dev:amd64 (11.4.0-1ubuntu1~22.04) ...\n",
      "Setting up cpp (4:11.2.0-1ubuntu1) ...\n",
      "Setting up gnupg (2.2.27-3ubuntu2.1) ...\n",
      "Setting up binutils-x86-64-linux-gnu (2.38-4ubuntu2.6) ...\n",
      "Setting up binutils (2.38-4ubuntu2.6) ...\n",
      "Setting up dpkg-dev (1.21.1ubuntu2.3) ...\n",
      "Setting up libstdc++-11-dev:amd64 (11.4.0-1ubuntu1~22.04) ...\n",
      "Setting up gcc-11 (11.4.0-1ubuntu1~22.04) ...\n",
      "Setting up g++-11 (11.4.0-1ubuntu1~22.04) ...\n",
      "Setting up gcc (4:11.2.0-1ubuntu1) ...\n",
      "Setting up g++ (4:11.2.0-1ubuntu1) ...\n",
      "update-alternatives: using /usr/bin/g++ to provide /usr/bin/c++ (c++) in auto mode\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/c++.1.gz because associated file /usr/share/man/man1/g++.1.gz (of link group c++) doesn't exist\n",
      "Setting up build-essential (12.9ubuntu3) ...\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n"
     ]
    }
   ],
   "source": [
    "!apt-get install build-essential -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.6.0+cu126\n",
      "Uninstalling torch-2.6.0+cu126:\n",
      "  Successfully uninstalled torch-2.6.0+cu126\n",
      "Found existing installation: torchvision 0.21.0+cu126\n",
      "Uninstalling torchvision-0.21.0+cu126:\n",
      "  Successfully uninstalled torchvision-0.21.0+cu126\n",
      "Found existing installation: torchaudio 2.6.0+cu126\n",
      "Uninstalling torchaudio-2.6.0+cu126:\n",
      "  Successfully uninstalled torchaudio-2.6.0+cu126\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0mLooking in indexes: https://download.pytorch.org/whl/cu126\n",
      "Collecting torch\n",
      "  Using cached https://download.pytorch.org/whl/cu126/torch-2.6.0%2Bcu126-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (28 kB)\n",
      "Collecting torchvision\n",
      "  Using cached https://download.pytorch.org/whl/cu126/torchvision-0.21.0%2Bcu126-cp311-cp311-linux_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting torchaudio\n",
      "  Using cached https://download.pytorch.org/whl/cu126/torchaudio-2.6.0%2Bcu126-cp311-cp311-linux_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.11/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /opt/conda/lib/python3.11/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /opt/conda/lib/python3.11/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /opt/conda/lib/python3.11/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /opt/conda/lib/python3.11/site-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /opt/conda/lib/python3.11/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /opt/conda/lib/python3.11/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /opt/conda/lib/python3.11/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /opt/conda/lib/python3.11/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /opt/conda/lib/python3.11/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /opt/conda/lib/python3.11/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.11/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /opt/conda/lib/python3.11/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /opt/conda/lib/python3.11/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: triton==3.2.0 in /opt/conda/lib/python3.11/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from torchvision) (2.2.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.11/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Using cached https://download.pytorch.org/whl/cu126/torch-2.6.0%2Bcu126-cp311-cp311-manylinux_2_28_x86_64.whl (764.6 MB)\n",
      "Using cached https://download.pytorch.org/whl/cu126/torchvision-0.21.0%2Bcu126-cp311-cp311-linux_x86_64.whl (7.3 MB)\n",
      "Using cached https://download.pytorch.org/whl/cu126/torchaudio-2.6.0%2Bcu126-cp311-cp311-linux_x86_64.whl (3.5 MB)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "Successfully installed torch-2.6.0+cu126 torchaudio-2.6.0+cu126 torchvision-0.21.0+cu126\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0mRequirement already satisfied: unsloth in /opt/conda/lib/python3.11/site-packages (2025.2.12)\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0mFound existing installation: unsloth 2025.2.12\n",
      "Uninstalling unsloth-2025.2.12:\n",
      "  Successfully uninstalled unsloth-2025.2.12\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0mCollecting unsloth@ git+https://github.com/unslothai/unsloth.git (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-gtrva92l/unsloth_666674fae1c4490399b87eb3ff9a42bc\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-gtrva92l/unsloth_666674fae1c4490399b87eb3ff9a42bc\n",
      "  Resolved https://github.com/unslothai/unsloth.git to commit d1d15f1d14f1168837d29b9c08e9b6d63945d469\n",
      "  Installing build dependencies ... \u001B[?25ldone\n",
      "\u001B[?25h  Getting requirements to build wheel ... \u001B[?25ldone\n",
      "\u001B[?25h  Preparing metadata (pyproject.toml) ... \u001B[?25ldone\n",
      "\u001B[?25hRequirement already satisfied: unsloth_zoo>=2025.2.5 in /opt/conda/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.2.5)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (24.1)\n",
      "Requirement already satisfied: tyro in /opt/conda/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.9.14)\n",
      "Requirement already satisfied: transformers!=4.47.0,>=4.46.1 in /opt/conda/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.49.0)\n",
      "Requirement already satisfied: datasets>=2.16.0 in /opt/conda/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.3.1)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.2.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.66.4)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (5.9.0)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /opt/conda/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.43.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.3)\n",
      "Requirement already satisfied: protobuf<4.0.0 in /opt/conda/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.20.3)\n",
      "Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.28.1)\n",
      "Requirement already satisfied: hf_transfer in /opt/conda/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.9)\n",
      "Requirement already satisfied: bitsandbytes>=0.43.3 in /opt/conda/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.45.2)\n",
      "Requirement already satisfied: torch<3,>=2.0 in /opt/conda/lib/python3.11/site-packages (from bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.6.0+cu126)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.11.12)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.11.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers!=4.47.0,>=4.46.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.11/site-packages (from transformers!=4.47.0,>=4.46.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers!=4.47.0,>=4.46.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.5.2)\n",
      "Requirement already satisfied: triton in /opt/conda/lib/python3.11/site-packages (from unsloth_zoo>=2025.2.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.2.0)\n",
      "Requirement already satisfied: accelerate>=0.34.1 in /opt/conda/lib/python3.11/site-packages (from unsloth_zoo>=2025.2.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.4.0)\n",
      "Requirement already satisfied: trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 in /opt/conda/lib/python3.11/site-packages (from unsloth_zoo>=2025.2.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.15.1)\n",
      "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from unsloth_zoo>=2025.2.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.14.0)\n",
      "Requirement already satisfied: cut_cross_entropy in /opt/conda/lib/python3.11/site-packages (from unsloth_zoo>=2025.2.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (25.1.1)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.11/site-packages (from unsloth_zoo>=2025.2.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (10.4.0)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /opt/conda/lib/python3.11/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.11/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (13.9.4)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /opt/conda/lib/python3.11/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.7.1)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /opt/conda/lib/python3.11/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.4.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.7.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.15.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.13.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.6.85)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.1.3)\n",
      "Building wheels for collected packages: unsloth\n",
      "  Building wheel for unsloth (pyproject.toml) ... \u001B[?25ldone\n",
      "\u001B[?25h  Created wheel for unsloth: filename=unsloth-2025.2.12-py3-none-any.whl size=187179 sha256=65d32b58d12dd8f4642264fccef89aadb128cb51739301464eda7861cf2c58f8\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-y1zp5nub/wheels/d1/17/05/850ab10c33284a4763b0595cd8ea9d01fce6e221cac24b3c01\n",
      "Successfully built unsloth\n",
      "Installing collected packages: unsloth\n",
      "Successfully installed unsloth-2025.2.12\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0mRequirement already satisfied: sacrebleu in /opt/conda/lib/python3.11/site-packages (2.5.1)\n",
      "Requirement already satisfied: portalocker in /opt/conda/lib/python3.11/site-packages (from sacrebleu) (3.1.1)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.11/site-packages (from sacrebleu) (2024.11.6)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.11/site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from sacrebleu) (2.2.3)\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.11/site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.11/site-packages (from sacrebleu) (5.3.1)\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0mRequirement already satisfied: pytest-playwright in /opt/conda/lib/python3.11/site-packages (0.7.0)\n",
      "Requirement already satisfied: playwright>=1.18 in /opt/conda/lib/python3.11/site-packages (from pytest-playwright) (1.50.0)\n",
      "Requirement already satisfied: pytest<9.0.0,>=6.2.4 in /opt/conda/lib/python3.11/site-packages (from pytest-playwright) (8.3.4)\n",
      "Requirement already satisfied: pytest-base-url<3.0.0,>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from pytest-playwright) (2.1.0)\n",
      "Requirement already satisfied: python-slugify<9.0.0,>=6.0.0 in /opt/conda/lib/python3.11/site-packages (from pytest-playwright) (8.0.4)\n",
      "Requirement already satisfied: pyee<13,>=12 in /opt/conda/lib/python3.11/site-packages (from playwright>=1.18->pytest-playwright) (12.1.1)\n",
      "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /opt/conda/lib/python3.11/site-packages (from playwright>=1.18->pytest-playwright) (3.1.1)\n",
      "Requirement already satisfied: iniconfig in /opt/conda/lib/python3.11/site-packages (from pytest<9.0.0,>=6.2.4->pytest-playwright) (2.0.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from pytest<9.0.0,>=6.2.4->pytest-playwright) (24.1)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /opt/conda/lib/python3.11/site-packages (from pytest<9.0.0,>=6.2.4->pytest-playwright) (1.5.0)\n",
      "Requirement already satisfied: requests>=2.9 in /opt/conda/lib/python3.11/site-packages (from pytest-base-url<3.0.0,>=1.0.0->pytest-playwright) (2.32.3)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /opt/conda/lib/python3.11/site-packages (from python-slugify<9.0.0,>=6.0.0->pytest-playwright) (1.3)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.11/site-packages (from pyee<13,>=12->playwright>=1.18->pytest-playwright) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.9->pytest-base-url<3.0.0,>=1.0.0->pytest-playwright) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.9->pytest-base-url<3.0.0,>=1.0.0->pytest-playwright) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.9->pytest-base-url<3.0.0,>=1.0.0->pytest-playwright) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.9->pytest-base-url<3.0.0,>=1.0.0->pytest-playwright) (2024.7.4)\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0mRequirement already satisfied: matplotlib in /opt/conda/lib/python3.11/site-packages (3.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0mRequirement already satisfied: pillow in /opt/conda/lib/python3.11/site-packages (10.4.0)\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0mRequirement already satisfied: torchvision in /opt/conda/lib/python3.11/site-packages (0.21.0+cu126)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from torchvision) (2.2.3)\n",
      "Requirement already satisfied: torch==2.6.0 in /opt/conda/lib/python3.11/site-packages (from torchvision) (2.6.0+cu126)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.11/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (4.11.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (12.6.85)\n",
      "Requirement already satisfied: triton==3.2.0 in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (3.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch==2.6.0->torchvision) (2.1.3)\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0mRequirement already satisfied: lpips in /opt/conda/lib/python3.11/site-packages (0.1.4)\n",
      "Requirement already satisfied: torch>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from lpips) (2.6.0+cu126)\n",
      "Requirement already satisfied: torchvision>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from lpips) (0.21.0+cu126)\n",
      "Requirement already satisfied: numpy>=1.14.3 in /opt/conda/lib/python3.11/site-packages (from lpips) (2.2.3)\n",
      "Requirement already satisfied: scipy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from lpips) (1.15.2)\n",
      "Requirement already satisfied: tqdm>=4.28.1 in /opt/conda/lib/python3.11/site-packages (from lpips) (4.66.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch>=0.4.0->lpips) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.11/site-packages (from torch>=0.4.0->lpips) (4.11.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.11/site-packages (from torch>=0.4.0->lpips) (1.13.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=0.4.0->lpips) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=0.4.0->lpips) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch>=0.4.0->lpips) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /opt/conda/lib/python3.11/site-packages (from torch>=0.4.0->lpips) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /opt/conda/lib/python3.11/site-packages (from torch>=0.4.0->lpips) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /opt/conda/lib/python3.11/site-packages (from torch>=0.4.0->lpips) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /opt/conda/lib/python3.11/site-packages (from torch>=0.4.0->lpips) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /opt/conda/lib/python3.11/site-packages (from torch>=0.4.0->lpips) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /opt/conda/lib/python3.11/site-packages (from torch>=0.4.0->lpips) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /opt/conda/lib/python3.11/site-packages (from torch>=0.4.0->lpips) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /opt/conda/lib/python3.11/site-packages (from torch>=0.4.0->lpips) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /opt/conda/lib/python3.11/site-packages (from torch>=0.4.0->lpips) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /opt/conda/lib/python3.11/site-packages (from torch>=0.4.0->lpips) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.11/site-packages (from torch>=0.4.0->lpips) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /opt/conda/lib/python3.11/site-packages (from torch>=0.4.0->lpips) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /opt/conda/lib/python3.11/site-packages (from torch>=0.4.0->lpips) (12.6.85)\n",
      "Requirement already satisfied: triton==3.2.0 in /opt/conda/lib/python3.11/site-packages (from torch>=0.4.0->lpips) (3.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch>=0.4.0->lpips) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.11/site-packages (from torchvision>=0.2.1->lpips) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=0.4.0->lpips) (2.1.3)\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0mInstalling dependencies...\n",
      "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
      "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
      "Fetched 384 kB in 1s (356 kB/s)  \n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "fonts-freefont-ttf is already the newest version (20120503-10build1).\n",
      "fonts-liberation is already the newest version (1:1.07.4-11).\n",
      "libasound2 is already the newest version (1.2.6.1-1ubuntu1).\n",
      "libatk-bridge2.0-0 is already the newest version (2.38.0-3).\n",
      "libatk1.0-0 is already the newest version (2.36.0-3build1).\n",
      "libatspi2.0-0 is already the newest version (2.44.0-3).\n",
      "libcairo-gobject2 is already the newest version (1.16.0-5ubuntu2).\n",
      "libcairo2 is already the newest version (1.16.0-5ubuntu2).\n",
      "libdbus-glib-1-2 is already the newest version (0.112-2build1).\n",
      "libegl1 is already the newest version (1.4.0-1).\n",
      "libenchant-2-2 is already the newest version (2.3.2-1ubuntu2).\n",
      "libepoxy0 is already the newest version (1.5.10-1).\n",
      "libevdev2 is already the newest version (1.12.1+dfsg-1).\n",
      "libevent-2.1-7 is already the newest version (2.1.12-stable-1build3).\n",
      "libfontconfig1 is already the newest version (2.13.1-4.2ubuntu5).\n",
      "libgles2 is already the newest version (1.4.0-1).\n",
      "libglx0 is already the newest version (1.4.0-1).\n",
      "libgudev-1.0-0 is already the newest version (1:237-2build1).\n",
      "libhyphen0 is already the newest version (2.8.8-7build2).\n",
      "libicu70 is already the newest version (70.1-2).\n",
      "libjpeg-turbo8 is already the newest version (2.1.2-0ubuntu1).\n",
      "liblcms2-2 is already the newest version (2.12~rc1-2build2).\n",
      "libmanette-0.2-0 is already the newest version (0.2.6-3build1).\n",
      "libopengl0 is already the newest version (1.4.0-1).\n",
      "libopus0 is already the newest version (1.3.1-0.1build2).\n",
      "libpng16-16 is already the newest version (1.6.37-3build5).\n",
      "libproxy1v5 is already the newest version (0.4.17-2).\n",
      "libsecret-1-0 is already the newest version (0.20.5-2).\n",
      "libwoff1 is already the newest version (1.0.2-1build4).\n",
      "libxcb-shm0 is already the newest version (1.14-3ubuntu3).\n",
      "libxcb1 is already the newest version (1.14-3ubuntu3).\n",
      "libxcomposite1 is already the newest version (1:0.4.5-1build2).\n",
      "libxcursor1 is already the newest version (1:1.2.0-2build4).\n",
      "libxdamage1 is already the newest version (1:1.1.5-2build2).\n",
      "libxext6 is already the newest version (2:1.3.4-1build1).\n",
      "libxfixes3 is already the newest version (1:6.0.0-1).\n",
      "libxi6 is already the newest version (2:1.8-1build1).\n",
      "libxkbcommon0 is already the newest version (1.4.0-1).\n",
      "libxrandr2 is already the newest version (2:1.5.2-1build1).\n",
      "libxrender1 is already the newest version (1:0.9.10-1build4).\n",
      "libxtst6 is already the newest version (2:1.2.3-1build4).\n",
      "xfonts-scalable is already the newest version (1:1.0.3-1.2ubuntu1).\n",
      "fonts-ipafont-gothic is already the newest version (00303-21ubuntu1).\n",
      "fonts-tlwg-loma-otf is already the newest version (1:0.7.3-1).\n",
      "fonts-unifont is already the newest version (1:14.0.01-1).\n",
      "fonts-wqy-zenhei is already the newest version (0.9.45-8).\n",
      "libavif13 is already the newest version (0.9.3-3).\n",
      "libffi7 is already the newest version (3.3-5ubuntu1).\n",
      "libx264-163 is already the newest version (2:0.163.3060+git5db6aa6-2build1).\n",
      "xfonts-cyrillic is already the newest version (1:1.0.5).\n",
      "fonts-noto-color-emoji is already the newest version (2.042-0ubuntu0.22.04.1).\n",
      "gstreamer1.0-plugins-base is already the newest version (1.20.1-1ubuntu0.4).\n",
      "gstreamer1.0-plugins-good is already the newest version (1.20.3-0ubuntu1.3).\n",
      "libatomic1 is already the newest version (12.3.0-1ubuntu1~22.04).\n",
      "libcups2 is already the newest version (2.4.1op1-1ubuntu4.11).\n",
      "libdbus-1-3 is already the newest version (1.12.20-2ubuntu4.1).\n",
      "libdrm2 is already the newest version (2.4.113-2~ubuntu0.22.04.1).\n",
      "libfreetype6 is already the newest version (2.11.1+dfsg-1ubuntu0.2).\n",
      "libgbm1 is already the newest version (23.2.1-1ubuntu3.1~22.04.3).\n",
      "libgdk-pixbuf-2.0-0 is already the newest version (2.42.8+dfsg-1ubuntu0.3).\n",
      "libglib2.0-0 is already the newest version (2.72.4-0ubuntu2.4).\n",
      "libgstreamer-gl1.0-0 is already the newest version (1.20.1-1ubuntu0.4).\n",
      "libgstreamer-plugins-base1.0-0 is already the newest version (1.20.1-1ubuntu0.4).\n",
      "libgstreamer1.0-0 is already the newest version (1.20.3-0ubuntu1.1).\n",
      "libgtk-3-0 is already the newest version (3.24.33-1ubuntu2.2).\n",
      "libgtk-4-1 is already the newest version (4.6.9+ds-0ubuntu0.22.04.2).\n",
      "libharfbuzz-icu0 is already the newest version (2.7.4-1ubuntu3.2).\n",
      "libharfbuzz0b is already the newest version (2.7.4-1ubuntu3.2).\n",
      "libnotify4 is already the newest version (0.7.9-3ubuntu5.22.04.1).\n",
      "libnspr4 is already the newest version (2:4.35-0ubuntu0.22.04.1).\n",
      "libnss3 is already the newest version (2:3.98-0ubuntu0.22.04.2).\n",
      "libopenjp2-7 is already the newest version (2.4.0-6ubuntu0.3).\n",
      "libpango-1.0-0 is already the newest version (1.50.6+ds-2ubuntu1).\n",
      "libpangocairo-1.0-0 is already the newest version (1.50.6+ds-2ubuntu1).\n",
      "libwayland-client0 is already the newest version (1.20.0-1ubuntu0.1).\n",
      "libwayland-egl1 is already the newest version (1.20.0-1ubuntu0.1).\n",
      "libwayland-server0 is already the newest version (1.20.0-1ubuntu0.1).\n",
      "libwebpdemux2 is already the newest version (1.2.2-2ubuntu0.22.04.2).\n",
      "libx11-6 is already the newest version (2:1.7.5-1ubuntu0.3).\n",
      "libx11-xcb1 is already the newest version (2:1.7.5-1ubuntu0.3).\n",
      "libxml2 is already the newest version (2.9.13+dfsg-1ubuntu0.5).\n",
      "libxslt1.1 is already the newest version (1.1.34-4ubuntu0.22.04.1).\n",
      "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
      "gstreamer1.0-libav is already the newest version (1.20.3-0ubuntu1).\n",
      "gstreamer1.0-plugins-bad is already the newest version (1.20.3-0ubuntu1.1).\n",
      "libsoup-3.0-0 is already the newest version (3.0.7-0ubuntu1).\n",
      "xvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.12).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 40 not upgraded.\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (2.2.3)\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0mRequirement already satisfied: tensorboard in /opt/conda/lib/python3.11/site-packages (2.19.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.11/site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.11/site-packages (from tensorboard) (1.70.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.11/site-packages (from tensorboard) (3.7)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard) (2.2.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from tensorboard) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /opt/conda/lib/python3.11/site-packages (from tensorboard) (3.20.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard) (69.5.1)\n",
      "Requirement already satisfied: six>1.9 in /opt/conda/lib/python3.11/site-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall torch torchvision torchaudio -y && pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\n",
    "\n",
    "!pip install unsloth\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "!pip install sacrebleu\n",
    "!pip install pytest-playwright\n",
    "!playwright install\n",
    "!pip install matplotlib\n",
    "!pip install pillow\n",
    "!pip install torchvision\n",
    "!pip install lpips\n",
    "\n",
    "!playwright install-deps  \n",
    "\n",
    "!pip install -U numpy\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae539acb48bf192b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.6.0+cu124 with CUDA 1204 (you have 2.6.0+cu126)\n",
      "    Python  3.11.11 (you have 3.11.9)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from transformers import TrainingArguments, TextStreamer\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import Dataset\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "# Saving model\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0b19b466d146db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 5020\n",
    "\n",
    "def load_model():\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=\"unsloth/Llama-3.2-1B-bnb-4bit\",\n",
    "        max_seq_length=max_seq_length,\n",
    "        load_in_4bit=True,\n",
    "        dtype=None,\n",
    "    )\n",
    "    \n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=16,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"],\n",
    "        use_rslora=True,\n",
    "        use_gradient_checkpointing=\"unsloth\",\n",
    "        random_state = 32,\n",
    "        loftq_config = None,\n",
    "    )\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "167425bb45a12eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trainer(model, tokenizer, training_data, max_steps):\n",
    "    training_arguments = SFTConfig(\n",
    "        learning_rate=5e-5,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=64,\n",
    "        num_train_epochs=40,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=1,\n",
    "        # max_steps=max_steps,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        warmup_steps=10,\n",
    "        output_dir=\"output\",\n",
    "        seed=0,\n",
    "        save_total_limit=3,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=max_seq_length,\n",
    "        dataset_num_proc=10,\n",
    "        packing=True,\n",
    "    )\n",
    "\n",
    "    if max_steps is not None:\n",
    "        training_arguments.max_steps = max_steps\n",
    "    \n",
    "    return SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=training_data,\n",
    "        args=training_arguments,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "954eacb08fd7d7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import JSONDecodeError\n",
    "import numpy as np\n",
    "from utils.similarity import calculate_metrics\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from PIL import Image\n",
    "import torch\n",
    "import json\n",
    "\n",
    "log_dir = 'output/runs'\n",
    "with open('size-color-text-page-compressed.html', 'r') as f:\n",
    "    html_template = f.read()\n",
    "\n",
    "def add_image_to_tensorboard(name, step, img_path):\n",
    "    image = Image.open(img_path)\n",
    "    image = image.convert('RGB')\n",
    "    image_array = np.array(image)\n",
    "    image_tensor = torch.from_numpy(image_array)\n",
    "    image_tensor = image_tensor.permute(2, 0, 1)\n",
    "    image_tensor = image_tensor.float() / 255.0\n",
    "    \n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "    writer.add_image(name, image_tensor, step)\n",
    "    \n",
    "def add_text_to_tensorboard(name, step, text):\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "    writer.add_text(name, text, step)\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip().replace('<unk>', '') for pred in preds]\n",
    "    labels = [[label.strip().replace('<unk>', '')] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def apply_to_templates(text, template):\n",
    "    try:\n",
    "        variables = json.loads(text)\n",
    "    except JSONDecodeError:\n",
    "        return None\n",
    "\n",
    "    if not isinstance(variables, dict):\n",
    "        return None\n",
    "    \n",
    "    for variable_name, variable_value in variables.items():\n",
    "        template = template.replace('{{' + variable_name + '}}', str(variable_value))\n",
    "\n",
    "    return template\n",
    "\n",
    "def compute_metrics(decoded_predictions, decoded_labels, steps):\n",
    "    similarity_scores = []\n",
    "    perceptual_losses = []\n",
    "    index = 1\n",
    "    \n",
    "    for prediction, label in zip(decoded_predictions, decoded_labels):\n",
    "        prediction = prediction.replace(tokenizer.eos_token, '')\n",
    "        \n",
    "        add_text_to_tensorboard(f'valid_{index}_label_text', steps, label)\n",
    "        add_text_to_tensorboard(f'valid_{index}_prediction_text', steps, prediction)\n",
    "        \n",
    "        applied_label = apply_to_templates(label, html_template)\n",
    "        applied_prediction = apply_to_templates(prediction, html_template)\n",
    "\n",
    "        if applied_label is None or applied_prediction is None:\n",
    "            metrics = None\n",
    "        else:\n",
    "            add_text_to_tensorboard(f'valid_{index}_label_text_applied', steps, applied_label)\n",
    "            add_text_to_tensorboard(f'valid_{index}_prediction_text_applied', steps, applied_prediction)\n",
    "            metrics = calculate_metrics(\n",
    "                applied_label, \n",
    "                applied_prediction\n",
    "            )\n",
    "        \n",
    "        if metrics is not None:\n",
    "            similarity_scores.append(metrics['similarity'])\n",
    "            perceptual_losses.append(metrics['perceptual_loss'])\n",
    "            \n",
    "            add_image_to_tensorboard(f'valid_{index}_expectation', steps, metrics['expected_screenshot_path'])\n",
    "            add_image_to_tensorboard(f'valid_{index}_prediction', steps, metrics['predicted_screenshot_path'])\n",
    "        \n",
    "        index += 1\n",
    "\n",
    "    results = {\n",
    "        \"similarity\": float(np.mean(similarity_scores)),\n",
    "        \"perceptual_loss\": float(np.mean(perceptual_losses)),\n",
    "    }\n",
    "    \n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "    writer.add_scalar('similarity', results['similarity'], steps)\n",
    "    writer.add_scalar('perceptual_loss', results['perceptual_loss'], steps)\n",
    "    \n",
    "    print(\"Similarity:\", results['similarity'])\n",
    "    print(\"Perceptual loss:\", results['perceptual_loss'])\n",
    "\n",
    "    return results\n",
    "\n",
    "def test_prediction(model, data, steps):\n",
    "    answers = []\n",
    "    labels = []\n",
    "    print(\"Generating predictions...\")\n",
    "    for row in data:\n",
    "        inputs = tokenizer(\n",
    "        [\n",
    "            data_prompt.format(\n",
    "                #instructions\n",
    "                row['svg'],\n",
    "                #answer\n",
    "                \"\",\n",
    "            )\n",
    "        ], return_tensors = \"pt\").to(\"cuda\")\n",
    "        \n",
    "        outputs = model.generate(**inputs, max_new_tokens = 5020, use_cache = True)\n",
    "        answer = tokenizer.batch_decode(outputs)\n",
    "        answers.append(answer[0].split(\"### Response:\")[-1])\n",
    "        labels.append(row['html'])\n",
    "\n",
    "    print(\"Computing metrics...\")\n",
    "    compute_metrics(answers, labels, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a9436b4-22de-4180-a7d6-9aad00934c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70c8b3c6163163d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  unzip\n",
      "The following NEW packages will be installed:\n",
      "  unzip zip\n",
      "0 upgraded, 2 newly installed, 0 to remove and 40 not upgraded.\n",
      "Need to get 350 kB of archives.\n",
      "After this operation, 930 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 unzip amd64 6.0-26ubuntu3.2 [175 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 zip amd64 3.0-12build2 [176 kB]\n",
      "Fetched 350 kB in 1s (337 kB/s)m\u001B[33m\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "\n",
      "\u001B7\u001B[0;23r\u001B8\u001B[1ASelecting previously unselected package unzip.\n",
      "(Reading database ... 36713 files and directories currently installed.)\n",
      "Preparing to unpack .../unzip_6.0-26ubuntu3.2_amd64.deb ...\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [  0%]\u001B[49m\u001B[39m [..........................................................] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 11%]\u001B[49m\u001B[39m [######....................................................] \u001B8Unpacking unzip (6.0-26ubuntu3.2) ...\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 22%]\u001B[49m\u001B[39m [############..............................................] \u001B8Selecting previously unselected package zip.\n",
      "Preparing to unpack .../zip_3.0-12build2_amd64.deb ...\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 33%]\u001B[49m\u001B[39m [###################.......................................] \u001B8Unpacking zip (3.0-12build2) ...\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 44%]\u001B[49m\u001B[39m [#########################.................................] \u001B8Setting up unzip (6.0-26ubuntu3.2) ...\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 56%]\u001B[49m\u001B[39m [################################..........................] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 67%]\u001B[49m\u001B[39m [######################################....................] \u001B8Setting up zip (3.0-12build2) ...\n",
      "\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 78%]\u001B[49m\u001B[39m [#############################################.............] \u001B8\u001B7\u001B[24;0f\u001B[42m\u001B[30mProgress: [ 89%]\u001B[49m\u001B[39m [###################################################.......] \u001B8\n",
      "\u001B7\u001B[0;24r\u001B8\u001B[1A\u001B[J--2025-02-19 09:46:14--  https://www.dropbox.com/scl/fi/or7eexwsl7s9ud8otg4y4/data-rb-size-color-text-bare.zip?rlkey=35kkqe2k0a4xorh8q6ow7c1in&dl=1\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.13.18, 2620:100:6057:18::a27d:d12\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.13.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://uc818ada44470596e22503aa9185.dl.dropboxusercontent.com/cd/0/inline/CkYaPlUjam72cnkMVaSf3F7pXEwFM6NHprWiF8X4UqRMakqoK2EDEjd-ERwYYjJB4FlNPtE8uaBNF7Mkgan0MuzznE2oKxtVnTzcLH6dpbMlBPQpO0jbRm5z4ek5rHN3g9c/file?dl=1# [following]\n",
      "--2025-02-19 09:46:15--  https://uc818ada44470596e22503aa9185.dl.dropboxusercontent.com/cd/0/inline/CkYaPlUjam72cnkMVaSf3F7pXEwFM6NHprWiF8X4UqRMakqoK2EDEjd-ERwYYjJB4FlNPtE8uaBNF7Mkgan0MuzznE2oKxtVnTzcLH6dpbMlBPQpO0jbRm5z4ek5rHN3g9c/file?dl=1\n",
      "Resolving uc818ada44470596e22503aa9185.dl.dropboxusercontent.com (uc818ada44470596e22503aa9185.dl.dropboxusercontent.com)... 162.125.13.15, 2620:100:6057:15::a27d:d0f\n",
      "Connecting to uc818ada44470596e22503aa9185.dl.dropboxusercontent.com (uc818ada44470596e22503aa9185.dl.dropboxusercontent.com)|162.125.13.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: /cd/0/inline2/CkbRf8vXgHeWZt-k4r7XPgGO_yYUzI6hTVF0hnJT9txV9e4APp6X9dEgm2gqcn8D14VK6gEwEmFuMgG4yfSqil8JdrdMJwRrGnv1-DNFmLIvoorqV4z5onb-54FUN0IK72pMbf23Sx9TnXG5mZ_BC2BypZi_mUQp5PnnZIBseZFx8KRchCwym0hZ9BNO9H7p1iR0zO-SHpoOIfVFXMisQm-yU7y-S9ylTyx_9QTyMN1wuQl2W--Vm9tRrjJkA950fUHxYqT9NuwCsIadJHlTjYFiN10b6YqCt5frgv5HkyypkrLzrCrkD7iPKnyGF_PA-ISQUpUyL2CnMg8tDSXLxRokvgClXUuzCRZwq7drQMLIHQ/file?dl=1 [following]\n",
      "--2025-02-19 09:46:16--  https://uc818ada44470596e22503aa9185.dl.dropboxusercontent.com/cd/0/inline2/CkbRf8vXgHeWZt-k4r7XPgGO_yYUzI6hTVF0hnJT9txV9e4APp6X9dEgm2gqcn8D14VK6gEwEmFuMgG4yfSqil8JdrdMJwRrGnv1-DNFmLIvoorqV4z5onb-54FUN0IK72pMbf23Sx9TnXG5mZ_BC2BypZi_mUQp5PnnZIBseZFx8KRchCwym0hZ9BNO9H7p1iR0zO-SHpoOIfVFXMisQm-yU7y-S9ylTyx_9QTyMN1wuQl2W--Vm9tRrjJkA950fUHxYqT9NuwCsIadJHlTjYFiN10b6YqCt5frgv5HkyypkrLzrCrkD7iPKnyGF_PA-ISQUpUyL2CnMg8tDSXLxRokvgClXUuzCRZwq7drQMLIHQ/file?dl=1\n",
      "Reusing existing connection to uc818ada44470596e22503aa9185.dl.dropboxusercontent.com:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 26054794 (25M) [application/binary]\n",
      "Saving to: ‘model.zip’\n",
      "\n",
      "model.zip           100%[===================>]  24.85M  11.2MB/s    in 2.2s    \n",
      "\n",
      "2025-02-19 09:46:19 (11.2 MB/s) - ‘model.zip’ saved [26054794/26054794]\n",
      "\n",
      "Archive:  model.zip\n",
      "  inflating: data-rb-size-color-text-bare/data-00000-of-00001.arrow  \n",
      "  inflating: data-rb-size-color-text-bare/__MACOSX/._data-00000-of-00001.arrow  \n",
      "  inflating: data-rb-size-color-text-bare/dataset_info.json  \n",
      "  inflating: data-rb-size-color-text-bare/__MACOSX/._dataset_info.json  \n",
      "  inflating: data-rb-size-color-text-bare/state.json  \n",
      "  inflating: data-rb-size-color-text-bare/__MACOSX/._state.json  \n"
     ]
    }
   ],
   "source": [
    "!apt install zip -y\n",
    "!rm -rf data-rb-size-color-text-bare\n",
    "!mkdir -p data-rb-size-color-text-bare\n",
    "!wget \"https://www.dropbox.com/scl/fi/or7eexwsl7s9ud8otg4y4/data-rb-size-color-text-bare.zip?rlkey=35kkqe2k0a4xorh8q6ow7c1in&dl=1\" -O model.zip\n",
    "!unzip model.zip -d data-rb-size-color-text-bare\n",
    "\n",
    "!rm -rf data-rb-validate\n",
    "!mkdir -p data-rb-validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43f3d9ae5b9b9f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['svg', 'html'],\n",
       "        num_rows: 99849\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['svg', 'html'],\n",
       "        num_rows: 4\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "dataset = load_from_disk('data-rb-size-color-text-bare')\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=4/len(dataset))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33bf3a2cdcc7b19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.2.12: Fast Llama patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    GPU: NVIDIA H100 NVL. Max memory: 93.111 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.2.12 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model()\n",
    "\n",
    "data_prompt = \"\"\"Your job is to take variable parameters extracted from an SVG file of a web design and convert it into a variable set of parameters of HTML and CSS markup and stylesheet that represents the design in pixel-perfect accuracy.\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "def formatting_prompt(examples):\n",
    "    inputs       = examples[\"svg\"]\n",
    "    outputs      = examples[\"html\"]\n",
    "    texts = []\n",
    "    for input_, output in zip(inputs, outputs):\n",
    "        text = data_prompt.format(input_, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f57abcb9-04a6-4ad6-91af-7fd09e7118ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f50fe74531347c2b6ecdde7766cce1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c17b606238d46cd955e8c2a8c23a5ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_data = dataset.map(formatting_prompt, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "335b25f6-18b9-4ac9-bad7-2d6ccfe2355b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['svg', 'html', 'text'],\n",
       "        num_rows: 99849\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['svg', 'html', 'text'],\n",
       "        num_rows: 4\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8289ee3c-34eb-4ca3-b69b-2e7a2828a418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e738293b27b1454f8db3c5c4417f1a69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36a2cc28b3e54d7f9eb5bff9417392a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "967a4641187b45e2a75a9e8e15099c88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab1dd5d50ba642e6b4d6535a20b360f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['svg', 'html', 'text', 'input_ids', 'attention_mask', 'length'],\n",
      "        num_rows: 99849\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['svg', 'html', 'text', 'input_ids', 'attention_mask', 'length'],\n",
      "        num_rows: 4\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def get_token_lengths(examples):\n",
    "    inputs = tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=False,  # Don't truncate yet\n",
    "        padding=False,     # Don't pad yet\n",
    "        return_length=True,\n",
    "    )\n",
    "\n",
    "    return inputs\n",
    "\n",
    "tokenized_data = training_data.map(get_token_lengths, batched=True)\n",
    "\n",
    "def filter_function(example):\n",
    "    return example['length'] <= max_seq_length\n",
    "\n",
    "filtered_data = tokenized_data.filter(filter_function)\n",
    "\n",
    "print(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90aeb44b-e661-4d7b-8a56-995d7e115332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f8deeb21a774fa9805c312e01f965c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f3aae1bb58d478799aca036edb5dd32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filtered_data = filtered_data.remove_columns([\"input_ids\", \"attention_mask\", \"length\"])\n",
    "filtered_data.save_to_disk('data-rb-size-color-text-bare-filtered-' + str(max_seq_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4f049b9-5e8c-4b01-aaa1-bf0b9024d654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['svg', 'html', 'text'],\n",
       "        num_rows: 99849\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['svg', 'html', 'text'],\n",
       "        num_rows: 4\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "filtered_data = load_from_disk('data-rb-size-color-text-bare-filtered-' + str(max_seq_length))\n",
    "\n",
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eae39d89303cb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/90 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 270\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17f606b37e134eb38688380a94b4c1b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cebc77bd9e741faae36ae53efded804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6212395b85d74ddbb31a8d8c09e97677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d65ecb7d525402ab8b9309f3961c142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 270\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='272' max='270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [270/270 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>272</td>\n",
       "      <td>0.019800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/90 [06:38<9:51:16, 398.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 271\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ed7991a2c1141a6b4a492aa3ce0cf6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "642f7db088d447bb9199daf5abd844bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16c2a25d72874ef6a62a9a2f2f8be26c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 271\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='273' max='271' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [271/271 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>273</td>\n",
       "      <td>0.012200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/90 [13:15<9:43:28, 397.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 272\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01542222d5154ef5bbb0601b61ec2f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4cb0d5279f8498a87784c63c5f09f8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 272\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='274' max='272' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [272/272 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>274</td>\n",
       "      <td>0.025600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/90 [19:49<9:33:57, 395.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 273\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df59910d3d834aa4ba88cdebeffb86dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0939f84eb2847f0ae998794dba9bae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 273\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='275' max='273' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [273/273 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.012300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/90 [26:22<9:25:35, 394.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 274\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13d3c6d8e664c29a9c310d1fe8d60cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe04af625744ad99d6f7130256875d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 274\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='276' max='274' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [274/274 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>276</td>\n",
       "      <td>0.016600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 5/90 [32:56<9:18:47, 394.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 275\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fb8ed5ea95e4c168c934e3849c9273f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f219cc74e1f84377926425c34aadee1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 275\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='277' max='275' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [275/275 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>277</td>\n",
       "      <td>0.012200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 6/90 [39:31<9:12:35, 394.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 276\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "433f7f15ffc64a908dca7b9f6ebfdcc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cef7b12a28cd410eb13b98a8c10d508b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 276\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='278' max='276' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [276/276 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>278</td>\n",
       "      <td>0.026500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 7/90 [46:02<9:04:29, 393.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 277\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4dc81a5c5b248f88b5b6fe974af6c6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf66597ed9f1463095e2050ad57fdeaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 277\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='279' max='277' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [277/277 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>0.027100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 8/90 [52:38<8:58:39, 394.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 278\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d46a8ec7db384d83a5c8a2a3b219dbd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ebbb4e624b247f58394e8d2f095c64d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 278\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='280' max='278' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [278/278 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.034300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 9/90 [59:13<8:52:27, 394.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 279\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4bb9bae2c2b4a97b9794f2128f7304b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6347841e2aa94848ad5ff2234bf6e3bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 279\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='281' max='279' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [279/279 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>281</td>\n",
       "      <td>0.019500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 10/90 [1:05:46<8:45:40, 394.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 280\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32a49485b41d4979b98aed0f9540dd69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1dfa3e25313486199e934e44fe9b1ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 280\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='282' max='280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [280/280 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>282</td>\n",
       "      <td>0.019700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 11/90 [1:12:51<8:51:21, 403.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 281\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f80ba779b774c3cb5de8a597ec159df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "067d5660a2d1405b95d8066449d1bcbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 281\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='283' max='281' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [281/281 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>283</td>\n",
       "      <td>0.019700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 12/90 [1:19:51<8:51:03, 408.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 282\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fab993e16984f9ba3d3a597ebed8f81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3722a3d899540968215ffb3d7189dd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 282\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='284' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [282/282 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>284</td>\n",
       "      <td>0.019700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 13/90 [1:26:51<8:48:34, 411.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 283\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d716465a24794a048928cbe1e4d6ebff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7005c525fe16433785df7b3c0e9815fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 283\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='285' max='283' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [283/283 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>0.012200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 14/90 [1:33:52<8:45:28, 414.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 284\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bb39404375e43b2b92a869cbf58a1c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff115e1b8efc4724a9b267d313f8e2f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 284\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='286' max='284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [284/284 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>286</td>\n",
       "      <td>0.012300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 15/90 [1:40:52<8:40:34, 416.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 285\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3da6dbf45ab047719b172d02e63ca306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e511b55e4434490998cafde64d737e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 285\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='287' max='285' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [285/285 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>287</td>\n",
       "      <td>0.012200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 16/90 [1:47:52<8:34:44, 417.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 286\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba39d96fe24e4c39ae1faef5d6a53554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57c19e42063a4c499b95ab6194ac29db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 286\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='288' max='286' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [286/286 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.026300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 17/90 [1:54:54<8:29:20, 418.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 287\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08912a19db9d412d8ca0dc49a3e746a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b369430489d14ddaba4045fb1fedd909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 287\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='289' max='287' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [287/287 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>289</td>\n",
       "      <td>0.012200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 18/90 [2:01:56<8:23:46, 419.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 288\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "111fb5db7a4a4a7289b283ec5e98537f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91e9a45b712245729731c4f1341c23e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 288\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='290' max='288' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [288/288 00:01, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.019500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 19/90 [2:09:00<8:18:14, 421.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 289\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf48e0f9e5354c05853cdfd3bd3baa92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afc4869c288c4c65bca0e6c1ca3fdcb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 289\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='291' max='289' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [289/289 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>291</td>\n",
       "      <td>0.012300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 20/90 [2:15:37<8:02:52, 413.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 290\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78b039cf2c7943a68d0faa30d1c92fb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb2b957a81c645159b656f193b1c32c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 290\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='292' max='290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [290/290 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>292</td>\n",
       "      <td>0.012300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 21/90 [2:22:39<7:58:49, 416.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 291\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05258dae488443a99c452c0ba6bc233f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da1d2c5decf94a05af743234dd002cd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 291\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='293' max='291' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [291/291 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>293</td>\n",
       "      <td>0.012200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 22/90 [2:29:40<7:53:17, 417.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 292\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35bcf7497d941818f3ad61accdf8592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14713c8caff2444fa17fe6ecad7a5754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 292\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='294' max='292' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [292/292 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>294</td>\n",
       "      <td>0.020200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 23/90 [2:36:40<7:47:07, 418.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 293\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8d79dd364824c56a340d37dc8d075fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48960dd8b9c449c0983e4756c1c31608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 293\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='295' max='293' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [293/293 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>0.034000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 24/90 [2:43:43<7:41:50, 419.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 294\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b79e34698c4971b98f7de1d5e50cd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f22c5740f4b4131849acc302ed6ba93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 294\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='296' max='294' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [294/294 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>296</td>\n",
       "      <td>0.019400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 25/90 [2:50:48<7:36:17, 421.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 295\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11dc9236af5c4c58bb224fef2dd738bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e83381f4eb1d4803a6cfa589e89af88c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 295\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='297' max='295' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [295/295 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>297</td>\n",
       "      <td>0.019600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 26/90 [2:57:50<7:29:32, 421.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 296\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "641c6d7e026243f685c0eeec253db632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c720ab7edda840c2a8366d7991e6c407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 296\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='298' max='296' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [296/296 00:03, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>298</td>\n",
       "      <td>0.019500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 27/90 [3:04:53<7:23:12, 422.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 297\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "546f04ef83b74eb58aa4649ff25be864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31eea07ac8e04ba7892d4cc5abb8dd8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 297\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='299' max='297' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [297/297 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>299</td>\n",
       "      <td>0.012300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 28/90 [3:11:56<7:16:12, 422.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 298\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19733c1e77124d9c868dc5bdd3f241df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa3d42f040584af4b7ad6fa62012340a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 298\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='298' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [298/298 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.019800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 29/90 [3:19:00<7:09:50, 422.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 299\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f2e10300db9484694a27ae4a1343a0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4257e51d9b64481b9d4c3e920fc071bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 299\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='301' max='299' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [299/299 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>301</td>\n",
       "      <td>0.027300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 30/90 [3:26:00<7:01:57, 421.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 300\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c49bbb657284e2da6d9b571d8492c45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df76aaef6da24eff82e806c008d2c980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 300\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='302' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>302</td>\n",
       "      <td>0.027300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 31/90 [3:33:00<6:54:23, 421.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 301\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b0acb9ec02645fd8afd24d84bd95165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ba16fbd047d40839d5787adcfaa29e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 301\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='303' max='301' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [301/301 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>303</td>\n",
       "      <td>0.026400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 32/90 [3:40:01<6:47:20, 421.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 302\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b507f3670a894ddd8288d33490a4c884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f50772f405ac4a94a6762558c3d2f47f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 302\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='304' max='302' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [302/302 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>0.027000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 33/90 [3:47:01<6:39:56, 420.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 303\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30e83ff48f2c45cab77afa3b803affa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87bf46d52774483988c067b3d46d1476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 303\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='305' max='303' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [303/303 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>0.027500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 34/90 [3:54:01<6:32:37, 420.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 304\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b8d3c8b61b41d6abd256faedc4623e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b2de2d1fd9a4015aa78ca0ba70bfdf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 304\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='306' max='304' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [304/304 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>306</td>\n",
       "      <td>0.019700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 35/90 [4:01:03<6:25:53, 420.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 305\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a570a9624b44abbf051936cbdfe9c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6212b2dae7346f9ba367eed6833c4f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 305\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='307' max='305' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [305/305 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>307</td>\n",
       "      <td>0.012200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 36/90 [4:08:03<6:18:43, 420.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 306\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e053a4385f844a02afffad06af0ec592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dc90bf31d6d44e0bd226ed246608124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 306\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='308' max='306' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [306/306 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>308</td>\n",
       "      <td>0.019800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 37/90 [4:15:04<6:11:39, 420.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 307\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a419959be1a042249524332fbf641326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ae17f8bc104bb2b1498184c68f0868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 307\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='309' max='307' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [307/307 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>309</td>\n",
       "      <td>0.027000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 38/90 [4:22:05<6:04:37, 420.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 308\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f392fe6a41f42e38f185b859c1dc9d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59ebbf54fe274b3f87e0e02a820512e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 308\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='310' max='308' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [308/308 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.019700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 39/90 [4:29:07<5:58:01, 421.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 309\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe244f0b22a5465e9768f00f054fa5d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc267882728945f5adc25d027a6cdbf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 309\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='311' max='309' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [309/309 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>311</td>\n",
       "      <td>0.019400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 40/90 [4:36:10<5:51:24, 421.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 310\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c4bb633037749d684d5e3929a1afa9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f32327f1b565431dbefe7b62e35295f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 310\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='312' max='310' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [310/310 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>312</td>\n",
       "      <td>0.026800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 41/90 [4:43:12<5:44:26, 421.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 311\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d07cb5ca2cd4b45a83769a2d80cf86b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4eaa7b1fedd482b9d1b2aed99f73115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 311\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='313' max='311' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [311/311 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>313</td>\n",
       "      <td>0.012200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 42/90 [4:49:48<5:31:11, 413.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 312\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec61de9a608f4849a8be07e8a2c54e00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5784c7cc3134a90a2800cd42c98a4c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 312\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='314' max='312' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [312/312 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>314</td>\n",
       "      <td>0.019900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 43/90 [4:56:25<5:20:23, 409.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 313\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35a087d0d7bf415e9bcf026cc74c6d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34923c7ff3774ecdafc679829b3c7d78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 313\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='315' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [313/313 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>0.012200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 44/90 [5:03:02<5:10:48, 405.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 314\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bf4617adbc040ea944c146b468bbb2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c13d27d2f894cd69e27868522be3f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 314\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='316' max='314' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [314/314 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>316</td>\n",
       "      <td>0.019600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 45/90 [5:09:36<5:01:29, 402.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 315\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37274f70c56a4894b6ba9a16b79ddbd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b8b7b5e6ea946cf9c818c2f9ba0fa8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 315\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='317' max='315' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [315/315 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>317</td>\n",
       "      <td>0.019300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 46/90 [5:16:12<4:53:21, 400.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 316\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d206edd9e85043fe986c6889c00903dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "560e4491504e4de2aabec45d42fdaaa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 316\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='318' max='316' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [316/316 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>318</td>\n",
       "      <td>0.026800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 47/90 [5:22:50<4:46:19, 399.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 317\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd63bb81a7ef4c3a9db76b7b66c9f628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3515fa7bb3847b7a0823c116022b50d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 317\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='319' max='317' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [317/317 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>319</td>\n",
       "      <td>0.019800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 48/90 [5:29:28<4:39:25, 399.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 318\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfd359b20d764fb88474b98ba2053067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c30edfac9d0a4da1b0502c45e78cca5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 318\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='318' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [318/318 00:01, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.012300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 49/90 [5:36:05<4:32:19, 398.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 319\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e7be688d344e81a64e268b096a4f64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffcdcb6931cc4e0f8a192de05171d1da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 319\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='321' max='319' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [319/319 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>321</td>\n",
       "      <td>0.019800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 50/90 [5:42:40<4:24:54, 397.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 320\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85c1d8fc2a5c4eb4a65df2f23735348a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72145900a6a54f4599ab020c2e1b91df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 320\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='322' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>322</td>\n",
       "      <td>0.019400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 51/90 [5:49:15<4:17:48, 396.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 321\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a772425aec9e49fc835595c924f393e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d031678a6abe491f9563d80866a9b1c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 321\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='323' max='321' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [321/321 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>323</td>\n",
       "      <td>0.012300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 52/90 [5:55:46<4:10:06, 394.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 322\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc96e974e39649a49ee4efa6a669611e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e10f0d742cd6463e96dfb95a7eb539d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 322\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='324' max='322' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [322/322 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>324</td>\n",
       "      <td>0.019900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 53/90 [6:02:15<4:02:28, 393.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 323\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "991f93de3ea9450b97789be05ff5ba2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba7335b8cd084df6a88a2c0f6703daca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 323\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='325' max='323' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [323/323 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.012200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 54/90 [6:08:49<3:56:08, 393.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 324\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ec36862cb8e4c5cbb2b6637319d8147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ad419275bcd461299ac9e9b95227088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 324\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='326' max='324' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [324/324 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>326</td>\n",
       "      <td>0.012300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 55/90 [6:15:24<3:49:45, 393.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 325\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e1c14712b264de6aa7d99da0ac21460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d16c61a78dcd492997f2e73d2e373918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 325\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='327' max='325' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [325/325 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>327</td>\n",
       "      <td>0.019400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 56/90 [6:21:56<3:42:52, 393.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 326\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a222a52a650c4732bbb1af5bda351495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cc3ea6393e64586b25be287d986b755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 326\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='328' max='326' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [326/326 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>328</td>\n",
       "      <td>0.027200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 57/90 [6:28:30<3:36:27, 393.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 327\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32dca48c00f645f48b5c56385cf68a57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b808cb6867974f63b73b252f82f13867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 327\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='329' max='327' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [327/327 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>329</td>\n",
       "      <td>0.019900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 58/90 [6:35:07<3:30:24, 394.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 328\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e27bc4cc2642e6a7d11cffc85dca82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ffdbcd1aba4af580f4a3f6c4f80c25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 328\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='330' max='328' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [328/328 00:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.019700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 59/90 [6:41:42<3:23:56, 394.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 329\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fe8db212b6e4d169e3de5b2b76ce690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63662a480b0247febe8abbf1ce80875e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 329\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='330' max='329' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [330/329 : < :, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 60/90 [6:47:51<3:13:29, 386.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 330\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bfc9f8f9dc34684a1d06148cb7199b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f23f30b280d4bb399bf845a73aebf5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 330\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='330' max='330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [330/330 : < :, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 61/90 [6:53:56<3:03:50, 380.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 331\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1cd2a6294a941a3b02521d752ff74f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3e713fabe7c43549276295dbaf4cd48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 4\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 331\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='331' max='331' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [331/331 00:00, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>331</td>\n",
       "      <td>0.012200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 62/90 [7:00:39<3:00:45, 387.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 332\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb1c003761e94cfca9fd16a27334c633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb582ce3f3a54381a695fa288e62d50f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 4\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 332\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='332' max='332' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [332/332 00:00, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>332</td>\n",
       "      <td>0.034600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 63/90 [7:07:13<2:55:10, 389.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d2754519a6d49bd8fe576a352d94f06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6ddf00732284dfb8c89f383fab12565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 4\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 333\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='333' max='333' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [333/333 00:00, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>333</td>\n",
       "      <td>0.019800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 64/90 [7:13:48<2:49:23, 390.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 334\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be60b1a05d134de0b23fbf18eadc201c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f070ee45a73b45419e436dba9ced7b2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 4\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 334\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='334' max='334' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [334/334 00:10, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>334</td>\n",
       "      <td>0.019500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 65/90 [7:20:32<2:44:33, 394.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 335\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0243216cda6c4016a52f85bfceeee574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a03407fa78c4ba4a6ade5271ddc7e84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 4\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 335\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='335' max='335' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [335/335 00:00, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>335</td>\n",
       "      <td>0.026700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 66/90 [7:27:08<2:38:01, 395.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 336\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8acbb1acba3546b18e35fbc88f0be2ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "521055ba3d3f48ec9d651b652ad0ff60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 4\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 336\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='336' max='336' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [336/336 00:00, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>336</td>\n",
       "      <td>0.019900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 67/90 [7:33:44<2:31:37, 395.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 337\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e97657c852ad4f559895ccdae37a8805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea0f8f9dda046c88560501dd03f4aa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 4\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 337\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='337' max='337' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [337/337 00:00, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>337</td>\n",
       "      <td>0.034800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 68/90 [7:40:22<2:25:17, 396.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 338\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "626834a8e7564632b798f597c3b8044b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8919443dc2184ccbbb5a3758583c77fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 4\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 338\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='338' max='338' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [338/338 00:00, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>338</td>\n",
       "      <td>0.012300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 69/90 [7:46:57<2:18:34, 395.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 339\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6e7fa892b6c45d88bd5e269529905b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d7267ea3bbd474583b338b8b9832126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 4\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 339\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='339' max='339' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [339/339 00:00, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>339</td>\n",
       "      <td>0.012200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 70/90 [7:53:29<2:11:33, 394.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 340\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89d4ed7819c14ca3bb5c920559711a8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "021419c141a0493cba68ece4782b200d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 4\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 340\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='340' max='340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [340/340 00:00, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.019900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 71/90 [8:00:03<2:04:51, 394.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 341\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7652ae3f7cd24ae89b5b94768dc20d6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d823b792c2944a6981a6b6907c3be0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 4\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 341\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='341' max='341' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [341/341 00:00, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>341</td>\n",
       "      <td>0.012100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 72/90 [8:06:47<1:59:12, 397.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 342\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3664febf56b94b659abdeff193f30b9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41a42cd9a1a340dbbbf7eedaf55f3ccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 4\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 342\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='342' max='342' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [342/342 00:00, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>342</td>\n",
       "      <td>0.019300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 73/90 [8:13:27<1:52:49, 398.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 343\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a963e9b76bb040d0b4528dc175304d4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dab97458e6e4379898e5f83e06cc1a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 4\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 343\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='343' max='343' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [343/343 00:00, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>343</td>\n",
       "      <td>0.027700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 74/90 [8:20:08<1:46:22, 398.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 344\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79d2e6880e1e42e1a91175b62749c562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "964c8ce461514d6e99404a27427844ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 4\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 344\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='344' max='344' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [344/344 00:00, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>344</td>\n",
       "      <td>0.012300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 75/90 [8:26:43<1:39:28, 397.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 345\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c50cc6a54b48c186e87a4a02eb916b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfb5374da80d4d8788d6d3fa512cb613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 4\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 345\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='345' max='345' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [345/345 00:00, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>345</td>\n",
       "      <td>0.019800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 76/90 [8:33:19<1:32:42, 397.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 346\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "443293b396104aa8b3ff9e9ac25a9778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a40d82282785437a8d2367e461bfea77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 4\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 346\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='346' max='346' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [346/346 00:00, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>346</td>\n",
       "      <td>0.012300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 77/90 [8:39:58<1:26:10, 397.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 347\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ba56e437d9d401b832819cfab1f7908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13b705d914e5453aaaa1360583ff69d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 4\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 347\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='347' max='347' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [347/347 00:00, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>347</td>\n",
       "      <td>0.016500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 78/90 [8:46:35<1:19:30, 397.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 348\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f76e6338555a40ab8c83de7c9794f3ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cafe8b5c7f64ee9a111309787b99a3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 4\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 348\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='348' max='348' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [348/348 00:00, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>348</td>\n",
       "      <td>0.012200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 79/90 [8:53:11<1:12:48, 397.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 349\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9752e87c46884386990a36993da9cc4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9cc1d7a10fb499aad4b7c0e3987150b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 4\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 349\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='349' max='349' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [349/349 00:00, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>349</td>\n",
       "      <td>0.026600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 80/90 [8:59:50<1:06:14, 397.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 350\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea0b0dff724f4c918c350c16b7076d93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a7f47c0166e439780d907abdf5f24d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 4\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 350\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [350/350 00:00, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.012300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 81/90 [9:06:29<59:43, 398.16s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 351\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cf03b4732824de8907600d9104beef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88a8cb77d41748df94f1b95dcdb7d507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 4\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 351\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [351/351 00:00, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>351</td>\n",
       "      <td>0.019900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 82/90 [9:13:04<52:57, 397.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 352\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc3a9e38a3da4e7d81f2463a0466833d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "585274309c234b87bfe506d03b13e002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 4\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 352\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='352' max='352' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [352/352 00:00, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>0.012200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 83/90 [9:19:50<46:38, 399.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 353\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3e333567cea4250a0c75d1ba0ad122a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d45aa6edc75a47f39d1a9f1e9a67ab3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 4\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 353\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='353' max='353' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [353/353 00:00, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>353</td>\n",
       "      <td>0.012200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 84/90 [9:26:34<40:05, 400.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 354\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31df315cb1984038b7001bfbc642d972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a166ace2832465fb3e126c043d45e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 4\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 354\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='354' max='354' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [354/354 00:00, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>354</td>\n",
       "      <td>0.034300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 85/90 [9:33:16<33:27, 401.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 355\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4307afc9218249df9c90d1eb1854879e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb6574c4b72c4841b934257057d0b86f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 4\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 355\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='355' max='355' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [355/355 00:00, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>355</td>\n",
       "      <td>0.019600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 86/90 [9:40:00<26:48, 402.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 356\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ea641dac47488b8c42992c78ba05e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c96a41e9cb84d7589256d89a074abe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 4\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 356\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='356' max='356' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [356/356 00:00, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>356</td>\n",
       "      <td>0.019700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 87/90 [9:46:45<20:08, 402.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 357\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3e7b8e9cb374b9faa0e0ab49e755082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6de983a202fa47839d350bfbcba47a78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 4\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 357\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='357' max='357' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [357/357 00:00, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>357</td>\n",
       "      <td>0.012200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 88/90 [9:53:27<13:25, 402.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 358\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b2f98b7a6824491aaa6a697fa86e716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3811d8cb214e4f96bd801f2ff15688ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 4\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 358\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='358' max='358' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [358/358 00:00, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>358</td>\n",
       "      <td>0.018900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 89/90 [10:00:12<06:43, 403.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n",
      "Steps: 359\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d70672545449474c85ec044f0142e3d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb1b55192da4485388fb7aec60ef68e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=10):   0%|          | 0/99849 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 14,180 | Num Epochs = 4\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 128 | Total steps = 359\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='359' max='359' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [359/359 00:00, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>359</td>\n",
       "      <td>0.027800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [10:06:54<00:00, 404.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "#resume = False\n",
    "resume = True\n",
    "\n",
    "for steps in tqdm(range(270, 360, 1)):\n",
    "    print(f\"Steps: {steps}\")\n",
    "\n",
    "    if steps > 0:\n",
    "        os.environ['UNSLOTH_RETURN_LOGITS'] = '1'\n",
    "        trainer = create_trainer(model, tokenizer, filtered_data['train'], steps)\n",
    "        if resume:\n",
    "            trainer.train(resume_from_checkpoint=True)\n",
    "        else:\n",
    "            trainer.train()\n",
    "            resume = True\n",
    "        \n",
    "    model = FastLanguageModel.for_inference(model)\n",
    "\n",
    "    results = test_prediction(model, filtered_data['test'], steps)\n",
    "\n",
    "    if results is not None and results['perceptual_loss'] == 0.0:\n",
    "        break\n",
    "\n",
    "    model = FastLanguageModel.for_training(model)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14378b83-4627-4812-aebd-07bdcc899bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"FONT_SIZE4\": \"154%\", \"COLOR8\": \"#c9a38e\", \"COLOR7\": \"#24d3e9\", \"SIZE3\": \"210px\", \"FONT_SIZE3\": \"4em\", \"COLOR6\": \"#05fd00\", \"COLOR5\": \"#44d3d6\", \"SIZE2\": \"35vw\", \"FONT_SIZE2\": \"19pt\", \"COLOR4\": \"#f06b18\", \"COLOR3\": \"#6bc83a\", \"FONT_SIZE1\": \"11px\", \"COLOR2\": \"#aa5044\", \"SIZE1\": \"74vh\", \"COLOR1\": \"#fcb18a\", \"WORD4\": \"OCCUR\", \"WORD3\": \"GLASS\", \"WORD2\": \"REACH\", \"WORD1\": \"POUND\"}\n",
      "Answer of the question is: \n",
      "{\"WORD4\": \"OCCUR\", \"LENGTH4\": \"92.875\", \"LENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTHLENGTH\n"
     ]
    }
   ],
   "source": [
    "test_index = 0\n",
    "text = filtered_data['test'][test_index]['svg']\n",
    "model = FastLanguageModel.for_inference(model)\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    data_prompt.format(\n",
    "        #instructions\n",
    "        text,\n",
    "        #answer\n",
    "        \"\",\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 5020, use_cache = True)\n",
    "answer=tokenizer.batch_decode(outputs)\n",
    "answer = answer[0].split(\"### Response:\")[-1]\n",
    "\n",
    "print(filtered_data['test'][test_index]['html'])\n",
    "print(\"Answer of the question is:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f2df0f9-6c3c-4024-99b0-f6d7832875fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n",
      "Computing metrics...\n",
      "Similarity: nan\n",
      "Perceptual loss: nan\n"
     ]
    }
   ],
   "source": [
    "test_prediction(model, filtered_data['test'], steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4065cf71-3f41-4bb8-bf51-4faef98067ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
